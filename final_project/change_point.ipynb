{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9ac37e",
   "metadata": {},
   "source": [
    "## WORK IN PROGRESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae3b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import ta\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback,\n",
    "    CheckpointCallback,\n",
    ")\n",
    "from stable_baselines3.common.buffers import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3 import PPO, A2C, DDPG, SAC, TD3\n",
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import random\n",
    "import torch\n",
    "from scipy.spatial import KDTree\n",
    "#chage point stuff\n",
    "import ruptures as rpt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd96c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SER using change point detection\n",
    "# we treat changing/different market regimes as sequential tasks\n",
    "# labels that have the same tag as the current regime will have higher priority in long-term memory; when there is a change point the scoring changes entirely\n",
    "\n",
    "# considering combination of matching global distribution w/ random normal dist and matching regimes detected by center-point detection\n",
    "\n",
    "class SERReplayBuffer(BaseBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        priority_queue_size: int,\n",
    "        priority_queue_percent: float, # i.e. 10% use --> 0.1\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "        optimize_memory_usage: bool = False,\n",
    "        handle_timeout_termination: bool = True,\n",
    "    ):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n",
    "\n",
    "        # Adjust buffer size\n",
    "        self.buffer_size = max(buffer_size // n_envs, 1)\n",
    "\n",
    "        # Check that the replay buffer can fit into the memory\n",
    "        if psutil is not None:\n",
    "            mem_available = psutil.virtual_memory().available\n",
    "\n",
    "        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n",
    "        if optimize_memory_usage and handle_timeout_termination:\n",
    "            raise ValueError(\n",
    "                \"ReplayBuffer does not support optimize_memory_usage = True \"\n",
    "                \"and handle_timeout_termination = True simultaneously.\"\n",
    "            )\n",
    "        self.optimize_memory_usage = optimize_memory_usage\n",
    "\n",
    "        self.observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        if not optimize_memory_usage:\n",
    "            # When optimizing memory, `observations` contains also the next observation\n",
    "            self.next_observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        self.actions = np.zeros(\n",
    "            (self.buffer_size, self.n_envs, self.action_dim), dtype=self._maybe_cast_dtype(action_space.dtype)\n",
    "        )\n",
    "\n",
    "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        # Handle timeouts termination properly if needed\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "        self.handle_timeout_termination = handle_timeout_termination\n",
    "        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "\n",
    "        if psutil is not None:\n",
    "            total_memory_usage: float = (\n",
    "                self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "            )\n",
    "\n",
    "            if not optimize_memory_usage:\n",
    "                total_memory_usage += self.next_observations.nbytes\n",
    "\n",
    "            if total_memory_usage > mem_available:\n",
    "                # Convert to GB\n",
    "                total_memory_usage /= 1e9\n",
    "                mem_available /= 1e9\n",
    "                warnings.warn(\n",
    "                    \"This system does not have apparently enough memory to store the complete \"\n",
    "                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n",
    "                )\n",
    "        \n",
    "        self.priority_queue_size= priority_queue_size\n",
    "        self.long_term_memory =[]  # (score, (obs, next_obs, action, reward, done))\n",
    "        self.priority_queue_percent = priority_queue_percent\n",
    "        self.curr_trend = None        \n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        infos: list[dict[str, Any]],\n",
    "    ) -> None:\n",
    "        # Reshape needed when using multiple envs with discrete observations\n",
    "        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n",
    "        if isinstance(self.observation_space, spaces.Discrete):\n",
    "            obs = obs.reshape((self.n_envs, *self.obs_shape))\n",
    "            next_obs = next_obs.reshape((self.n_envs, *self.obs_shape))\n",
    "\n",
    "        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n",
    "        action = action.reshape((self.n_envs, self.action_dim))\n",
    "\n",
    "        # Copy to avoid modification by reference\n",
    "        self.observations[self.pos] = np.array(obs)\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs)\n",
    "        else:\n",
    "            self.next_observations[self.pos] = np.array(next_obs)\n",
    "\n",
    "        self.actions[self.pos] = np.array(action)\n",
    "        self.rewards[self.pos] = np.array(reward)\n",
    "        self.dones[self.pos] = np.array(done)\n",
    "\n",
    "        if self.handle_timeout_termination:\n",
    "            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n",
    "\n",
    "        self.pos += 1\n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "            self.pos = 0\n",
    "\n",
    "        # compute score using given strategy and push to priority queue\n",
    "        # will get rid of the lowest score among the ones stored in long term mem if we exceed memory limit\n",
    "        \n",
    "        trend_ids = [info.get(\"trend_id\", None) for info in infos]\n",
    "        for trend_id in trend_ids:\n",
    "            if trend_id is not None and (self.curr_trend is None or trend_id != self.curr_trend):\n",
    "                print(\"set_curr_trend\")\n",
    "                self.set_curr_trend(trend_id)\n",
    "                #break\n",
    "\n",
    "        idx = (self.pos - 1) % self.buffer_size\n",
    "        for env_idx in range(self.n_envs):\n",
    "            score = float(np.random.normal())  \n",
    "            heapq.heappush(self.long_term_memory, (score, idx, env_idx, self.curr_trend))\n",
    "        if len(self.long_term_memory) > self.priority_queue_size:\n",
    "            heapq.heappop(self.long_term_memory)\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None):\n",
    "        #print(\"sampling\")\n",
    "        long_size=int(batch_size*self.priority_queue_percent)\n",
    "        fifo_size=batch_size-long_size\n",
    "        \n",
    "        # sample from FIFO\n",
    "        if not self.optimize_memory_usage:\n",
    "            return super().sample(batch_size=batch_size, env=env)\n",
    "        # Do not sample the element with index `self.pos` as the transitions is invalid\n",
    "        # (we use only one array to store `obs` and `next_obs`)\n",
    "        if self.full:\n",
    "            fifo_inds = (np.random.randint(1, self.buffer_size, size=fifo_size) + self.pos) % self.buffer_size\n",
    "        else:\n",
    "            fifo_inds = np.random.randint(0, self.pos, size=fifo_size)\n",
    "\n",
    "        fifo_samples = self._get_samples(fifo_inds, env=env)\n",
    "\n",
    "        # sample from long term memoru\n",
    "        if len(self.long_term_memory) >= long_size:\n",
    "            sampled_long_mem = random.sample(self.long_term_memory, long_size)\n",
    "        else:\n",
    "            sampled_long_mem = self.long_term_memory\n",
    "\n",
    "        buffer_idxs = [idx for _, idx, env in sampled_long_mem]\n",
    "        env_idxs = [env for _, idx, env in sampled_long_mem]\n",
    "\n",
    "        long_obs = self.observations[buffer_idxs, env_idxs]\n",
    "        if self.optimize_memory_usage:\n",
    "            long_next_obs = self.observations[(np.array(buffer_idxs) + 1) % self.buffer_size, env_idxs]\n",
    "        else:\n",
    "            long_next_obs = self.next_observations[buffer_idxs, env_idxs]\n",
    "\n",
    "        long_actions = self.actions[buffer_idxs, env_idxs]\n",
    "        long_rewards = self.rewards[buffer_idxs, env_idxs].reshape(-1, 1)\n",
    "        long_dones = (self.dones[buffer_idxs, env_idxs] * (1 - self.timeouts[buffer_idxs, env_idxs])).reshape(-1, 1)\n",
    "\n",
    "        long_obs = self.to_torch(self._normalize_obs(long_obs, env))\n",
    "        long_next_obs = self.to_torch(self._normalize_obs(long_next_obs, env))\n",
    "        long_actions = self.to_torch(long_actions)\n",
    "        long_rewards = self.to_torch(self._normalize_reward(long_rewards, env))\n",
    "        long_dones = self.to_torch(long_dones)\n",
    "\n",
    "        # combine FIFO + long-term mem\n",
    "        obs = torch.cat([fifo_samples.observations, long_obs], dim=0)\n",
    "        next_obs = torch.cat([fifo_samples.next_observations, long_next_obs], dim=0)\n",
    "        actions = torch.cat([fifo_samples.actions, long_actions], dim=0)\n",
    "        rewards = torch.cat([fifo_samples.rewards, long_rewards], dim=0)\n",
    "        dones = torch.cat([fifo_samples.dones, long_dones], dim=0)\n",
    "\n",
    "        return ReplayBufferSamples(obs, actions, next_obs, dones, rewards)\n",
    "\n",
    "    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> ReplayBufferSamples:\n",
    "        # Sample randomly the env idx\n",
    "        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            next_obs = self._normalize_obs(self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :], env)\n",
    "        else:\n",
    "            next_obs = self._normalize_obs(self.next_observations[batch_inds, env_indices, :], env)\n",
    "\n",
    "        data = (\n",
    "            self._normalize_obs(self.observations[batch_inds, env_indices, :], env),\n",
    "            self.actions[batch_inds, env_indices, :],\n",
    "            next_obs,\n",
    "            # Only use dones that are not due to timeouts\n",
    "            # deactivated by default (timeouts is initialized as an array of False)\n",
    "            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n",
    "            self._normalize_reward(self.rewards[batch_inds, env_indices].reshape(-1, 1), env),\n",
    "        )\n",
    "        #print(\"sampling done\")\n",
    "        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "    \n",
    "    def set_curr_trend(self, trend_id):\n",
    "        print(\"in set_curr_trend\")\n",
    "        if self.curr_trend != trend_id:\n",
    "            self.curr_trend = trend_id\n",
    "            print(\"start rescore\")\n",
    "            self.rescore_mem()\n",
    "\n",
    "    def rescore_mem(self):\n",
    "        # resocre entire long term memory if the current trend does not match the trend on the stored \n",
    "        #print(\"rescoring memory for {self.current_trend_id}\")\n",
    "        updated_mem = []\n",
    "        for score, idx, env_idx, trend_idx in self.long_term_memory:\n",
    "            new_score = score\n",
    "            if trend_idx == self.curr_trend:\n",
    "                new_score*=1.5\n",
    "            else:\n",
    "                new_score*=0.5\n",
    "\n",
    "            updated_mem.append((new_score, idx, env_idx, trend_idx))\n",
    "        print(\"heapify\")\n",
    "        heapq.heapify(updated_mem)\n",
    "        print(\"heapify done\")\n",
    "        self.long_term_memory = updated_mem\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_cast_dtype(dtype: np.typing.DTypeLike) -> np.typing.DTypeLike:\n",
    "        \"\"\"\n",
    "        Cast `np.float64` action datatype to `np.float32`,\n",
    "        keep the others dtype unchanged.\n",
    "        See GH#1572 for more information.\n",
    "\n",
    "        :param dtype: The original action space dtype\n",
    "        :return: ``np.float32`` if the dtype was float64,\n",
    "            the original dtype otherwise.\n",
    "        \"\"\"\n",
    "        if dtype == np.float64:\n",
    "            return np.float32\n",
    "        return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65008018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/melisacivelekoglu/.cache/kagglehub/datasets/gratefuldata/intraday-stock-data-1-min-sp-500-200821/versions/1\n",
      "\n",
      "Files in S&P 500 dataset directory:\n",
      "['1_min_SPY_2008-2021.csv']\n",
      "Files saved to ./datasets/sp500\n",
      "DataFrame columns: ['Unnamed: 0', 'date', 'open', 'high', 'low', 'close', 'volume', 'barCount', 'average', 'datetime', 'volume_adi', 'volume_obv', 'volume_cmf', 'volume_fi', 'volume_em', 'volume_sma_em', 'volume_vpt', 'volume_vwap', 'volume_mfi', 'volume_nvi', 'volatility_bbm', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_bbp', 'volatility_bbhi', 'volatility_bbli', 'volatility_kcc', 'volatility_kch', 'volatility_kcl', 'volatility_kcw', 'volatility_kcp', 'volatility_kchi', 'volatility_kcli', 'volatility_dcl', 'volatility_dch', 'volatility_dcm', 'volatility_dcw', 'volatility_dcp', 'volatility_atr', 'volatility_ui', 'trend_macd', 'trend_macd_signal', 'trend_macd_diff', 'trend_sma_fast', 'trend_sma_slow', 'trend_ema_fast', 'trend_ema_slow', 'trend_vortex_ind_pos', 'trend_vortex_ind_neg', 'trend_vortex_ind_diff', 'trend_trix', 'trend_mass_index', 'trend_dpo', 'trend_kst', 'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_conv', 'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b', 'trend_stc', 'trend_adx', 'trend_adx_pos', 'trend_adx_neg', 'trend_cci', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up', 'trend_aroon_down', 'trend_aroon_ind', 'trend_psar_up', 'trend_psar_down', 'trend_psar_up_indicator', 'trend_psar_down_indicator', 'momentum_rsi', 'momentum_stoch_rsi', 'momentum_stoch_rsi_k', 'momentum_stoch_rsi_d', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr', 'momentum_ao', 'momentum_roc', 'momentum_ppo', 'momentum_ppo_signal', 'momentum_ppo_hist', 'momentum_pvo', 'momentum_pvo_signal', 'momentum_pvo_hist', 'momentum_kama', 'others_dr', 'others_dlr', 'others_cr']\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"debashis74017/algo-trading-data-nifty-100-data-with-indicators\")\n",
    "path_sp500 = kagglehub.dataset_download(\n",
    "    \"gratefuldata/intraday-stock-data-1-min-sp-500-200821\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Path to dataset files:\", path_sp500)\n",
    "\n",
    "print(\"\\nFiles in S&P 500 dataset directory:\")\n",
    "try:\n",
    "    files_sp500 = os.listdir(path_sp500)\n",
    "    print(files_sp500)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Directory not found at {path_sp500}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "local_dir = \"./datasets/sp500\"\n",
    "os.makedirs(local_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "for fname in files_sp500:\n",
    "    src = os.path.join(path_sp500, fname)\n",
    "    dst = os.path.join(local_dir, fname)\n",
    "    shutil.copy2(src, dst)  # Use copy2 to preserve metadata\n",
    "\n",
    "print(f\"Files saved to {local_dir}\")\n",
    "\n",
    "full_csv_path = os.path.join(path_sp500, files_sp500[0])\n",
    "output_path = \"sp500_with_ta_features.csv\"\n",
    "\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    df = pd.read_csv(output_path)\n",
    "    # Ensure datetime column is correctly parsed when loading from CSV\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "else:\n",
    "    df = pd.read_csv(full_csv_path) \n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    start_date = pd.Timestamp(\"2018-01-01\")\n",
    "    end_date = pd.Timestamp(\"2021-12-31 23:59:59\")\n",
    "    df = df[(df[\"datetime\"] >= start_date) & (df[\"datetime\"] <= end_date)]\n",
    "    df = df.sort_values(\"datetime\")\n",
    "\n",
    "    df = ta.add_all_ta_features(\n",
    "        df,\n",
    "        open=\"open\",\n",
    "        high=\"high\",\n",
    "        low=\"low\",\n",
    "        close=\"close\",\n",
    "        volume=\"volume\",\n",
    "        fillna=True,\n",
    "    )\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"DataFrame columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b94ddc",
   "metadata": {},
   "source": [
    "## Change point detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b39303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change point start\n",
      "change point done\n",
      "begin kmeans\n",
      "completed kmeans\n"
     ]
    }
   ],
   "source": [
    "# takes around 4 mins for me\n",
    "df = df.sort_values(\"datetime\")\n",
    "df[\"log_return\"] = np.log(df[\"close\"]).diff().fillna(0)\n",
    "\n",
    "print(\"change point start\")\n",
    "signal = df[\"log_return\"].values.reshape(-1, 1)\n",
    "# TODO: Pelt is taking too long to run for me, try running and see if its faster for u and also compare if it does give better results\n",
    "#alg = rpt.Pelt(model=\"l2\").fit(signal)\n",
    "#change_points = alg.predict(pen=10)\n",
    "\n",
    "alg = rpt.Binseg(model=\"l2\").fit(signal)\n",
    "change_points = alg.predict(n_bkps=30)\n",
    "print(\"change point done\")\n",
    "\n",
    "segment_features = []\n",
    "segment_idx = []\n",
    "start = 0\n",
    "for end in change_points:\n",
    "    segment = df.iloc[start:end]\n",
    "    features = {\n",
    "        \"mean_return\": segment[\"log_return\"].mean(),\n",
    "        \"volatility_log\": segment[\"log_return\"].std(),                \n",
    "        \"volatility_atr\": segment[\"volatility_atr\"].mean(), # range based vol\n",
    "        \"volatility_bbw\": segment[\"volatility_bbw\"].mean(), # band width\n",
    "        \"volatility_dcw\": segment[\"volatility_dcw\"].mean(), # high-low channel width\n",
    "        \"duration\": len(segment),\n",
    "    }\n",
    "    segment_features.append(list(features.values()))\n",
    "    segment_idx.append((start, end))\n",
    "    start = end\n",
    "\n",
    "print(\"begin kmeans\")\n",
    "# now that we found the main points of change for market trends, we can do clustering like KMeans to tag similar ones with same id\n",
    "features_array = np.array(segment_features)\n",
    "kmeans = KMeans(n_clusters=4, random_state=12) #since we predict theres 4 main market trends: bull, bear, high vol, low vol \n",
    "labels = kmeans.fit_predict(features_array)\n",
    "print(\"completed kmeans\")\n",
    "\n",
    "df[\"trend_id\"] = -1\n",
    "for (start, end), label in zip(segment_idx, labels):\n",
    "    df.iloc[start:end, df.columns.get_loc(\"trend_id\")] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462ec4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, stock_data, transaction_cost_percent=0.005):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        # Remove any empty DataFrames\n",
    "        self.stock_data = {\n",
    "            ticker: df for ticker, df in stock_data.items() if not df.empty\n",
    "        }\n",
    "        self.tickers = list(self.stock_data.keys())\n",
    "\n",
    "        if not self.tickers:\n",
    "            raise ValueError(\"All provided stock data is empty\")\n",
    "\n",
    "        # Calculate the size of one stock's data\n",
    "        sample_df = next(iter(self.stock_data.values()))\n",
    "        self.n_features = len(sample_df.columns)\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(len(self.tickers),), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation space: price data for each stock + balance + shares held + net worth + max net worth + current step\n",
    "        self.obs_shape = self.n_features * len(self.tickers) + 2 + len(self.tickers) + 2\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.obs_shape,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize account balance 1M$\n",
    "        self.initial_balance = 1000000\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.max_net_worth = self.initial_balance\n",
    "        self.shares_held = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_shares_sold = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_sales_value = {ticker: 0 for ticker in self.tickers}\n",
    "\n",
    "        # Set the current step\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Calculate the minimum length of data across all stocks\n",
    "        self.max_steps = max(0, min(len(df) for df in self.stock_data.values()) - 1)\n",
    "\n",
    "        # Transaction cost\n",
    "        self.transaction_cost_percent = transaction_cost_percent\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.max_net_worth = self.initial_balance\n",
    "        self.shares_held = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_shares_sold = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_sales_value = {ticker: 0 for ticker in self.tickers}\n",
    "        self.current_step = 0\n",
    "        return self._next_observation(), {}\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # initialize the frame\n",
    "        frame = np.zeros(self.obs_shape)\n",
    "\n",
    "        # Add stock data for each ticker\n",
    "        idx = 0\n",
    "        # Loop through each ticker\n",
    "        for ticker in self.tickers:\n",
    "            # Get the DataFrame for the current ticker\n",
    "            df = self.stock_data[ticker]\n",
    "            # If the current step is less than the length of the DataFrame, add the price data for the current step\n",
    "            if self.current_step < len(df):\n",
    "                frame[idx : idx + self.n_features] = df.iloc[self.current_step].values\n",
    "            # Otherwise, add the last price data available\n",
    "            elif len(df) > 0:\n",
    "                frame[idx : idx + self.n_features] = df.iloc[-1].values\n",
    "            # Move the index to the next ticker\n",
    "            idx += self.n_features\n",
    "\n",
    "        # Add balance, shares held, net worth, max net worth, and current step\n",
    "        frame[-4 - len(self.tickers)] = self.balance  # Balance\n",
    "        frame[-3 - len(self.tickers) : -3] = [\n",
    "            self.shares_held[ticker] for ticker in self.tickers\n",
    "        ]  # Shares held\n",
    "        frame[-3] = self.net_worth  # Net worth\n",
    "        frame[-2] = self.max_net_worth  # Max net worth\n",
    "        frame[-1] = self.current_step  # Current step\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def step(self, actions):\n",
    "        # update the current step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # check if we have reached the maximum number of steps\n",
    "        if self.current_step > self.max_steps:\n",
    "            return self._next_observation(), 0, True, False, {}\n",
    "\n",
    "        current_prices = {}\n",
    "        # Loop through each ticker and perform the action\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            # Get the current price of the stock\n",
    "            current_prices[ticker] = self.stock_data[ticker].iloc[self.current_step][\n",
    "                \"Close\"\n",
    "            ]\n",
    "            # get the action for the current ticker\n",
    "            action = actions[i]\n",
    "\n",
    "            if action > 0:  # Buy\n",
    "                # Calculate the number of shares to buy\n",
    "                shares_to_buy = int(self.balance * action / current_prices[ticker])\n",
    "                # Calculate the cost of the shares\n",
    "                cost = shares_to_buy * current_prices[ticker]\n",
    "                # Transaction cost\n",
    "                transaction_cost = cost * self.transaction_cost_percent\n",
    "                # Update the balance and shares held\n",
    "                self.balance -= cost + transaction_cost\n",
    "                # Update the total shares sold\n",
    "                self.shares_held[ticker] += shares_to_buy\n",
    "\n",
    "            elif action < 0:  # Sell\n",
    "                # Calculate the number of shares to sell\n",
    "                shares_to_sell = int(self.shares_held[ticker] * abs(action))\n",
    "                # Calculate the sale value\n",
    "                sale = shares_to_sell * current_prices[ticker]\n",
    "                # Transaction cost, fixed fees...\n",
    "                transaction_cost = sale * self.transaction_cost_percent\n",
    "                # Update the balance and shares held\n",
    "                self.balance += sale - transaction_cost\n",
    "                # Update the total shares sold\n",
    "                self.shares_held[ticker] -= shares_to_sell\n",
    "                # Update the shares sold\n",
    "                self.total_shares_sold[ticker] += shares_to_sell\n",
    "                # Update the total sales value\n",
    "                self.total_sales_value[ticker] += sale\n",
    "\n",
    "        # Calculate the net worth\n",
    "        self.net_worth = self.balance + sum(\n",
    "            self.shares_held[ticker] * current_prices[ticker] for ticker in self.tickers\n",
    "        )\n",
    "        # Update the max net worth\n",
    "        self.max_net_worth = max(self.net_worth, self.max_net_worth)\n",
    "        # Calculate the reward\n",
    "        reward = self.net_worth - self.initial_balance\n",
    "        # Check if the episode is done\n",
    "        done = self.net_worth <= 0 or self.current_step >= self.max_steps\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        # for trend_ids\n",
    "        trend_id = self.stock_data[ticker].iloc[self.current_step][\"trend_id\"]\n",
    "        info = {\"trend_id\": trend_id}\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        # Print the current step, balance, shares held, net worth, and profit\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Balance: {self.balance:.2f}\")\n",
    "        for ticker in self.tickers:\n",
    "            print(f\"{ticker} Shares held: {self.shares_held[ticker]}\")\n",
    "        print(f\"Net worth: {self.net_worth:.2f}\")\n",
    "        print(f\"Profit: {profit:.2f}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7eec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_spawner, eval_env_spawner, config):\n",
    "    print(\"Initializing...\")\n",
    "\n",
    "    # Unpack config\n",
    "    log_dir = config[\"log_dir\"]\n",
    "    n_envs = config[\"n_envs\"]\n",
    "    ckpt = config.get(\"checkpoint\", None)\n",
    "    policy_args = config[\"policy_args\"]\n",
    "    algo_kwargs = config[\"algo_kwargs\"]\n",
    "    checkpoint_freq = config[\"checkpoint_freq\"]\n",
    "    eval_freq = config[\"eval_freq\"]\n",
    "    n_eval_episodes = config[\"n_eval_episodes\"]\n",
    "    n_train_timesteps = config[\"n_train_timesteps\"]\n",
    "    verbose_training = config[\"verbose_training\"]\n",
    "\n",
    "    # Setup\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    env = VecMonitor(DummyVecEnv(\n",
    "        [env_spawner] * n_envs,\n",
    "    ))\n",
    "    eval_env = VecMonitor(DummyVecEnv([eval_env_spawner]))\n",
    "\n",
    "    if ckpt is None:\n",
    "        model = SAC(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=env,\n",
    "            policy_kwargs=policy_args,\n",
    "            **algo_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        model = SAC.load(\n",
    "            path=ckpt,\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "    #if isinstance(model.replay_buffer, SERReplayBuffer):    \n",
    "     #   model.replay_buffer.set_q_nets(model.q_net, model.q_net_target)\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=checkpoint_freq,\n",
    "        save_path=log_dir,\n",
    "        name_prefix=os.path.basename(log_dir),\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=eval_freq,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"Training...\")\n",
    "    model.learn(\n",
    "        total_timesteps=n_train_timesteps,\n",
    "        callback=[\n",
    "            eval_callback,\n",
    "            checkpoint_callback,\n",
    "        ],\n",
    "        log_interval=1 if verbose_training else None,\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=True if verbose_training else False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cef992",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"close\",\n",
    "    \"volume_obv\",\n",
    "    \"volatility_atr\",\n",
    "    \"trend_macd\",\n",
    "    \"trend_macd_signal\",\n",
    "    \"trend_adx\",\n",
    "    \"momentum_rsi\",\n",
    "    \"momentum_ao\",\n",
    "    \"trend_sma_fast\",\n",
    "    \"trend_sma_slow\",\n",
    "    \"trend_id\"\n",
    "]\n",
    "\n",
    "# Make sure your DataFrame has no NaNs in these columns (drop or fill as needed)\n",
    "df_selected = df[feature_cols].dropna().reset_index(drop=True)\n",
    "df_selected = df_selected.rename(columns={\"close\": \"Close\"})\n",
    "\n",
    "stock_data = { \"SPY\": df_selected }\n",
    "\n",
    "def make_env():\n",
    "    return StockTradingEnv(stock_data)\n",
    "\n",
    "def make_eval_env():\n",
    "    return StockTradingEnv(stock_data)  # could be different data if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c416cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    # TRAINING PARAMETERS\n",
    "    \"log_dir\": \"./train_logs/base\",\n",
    "    \"n_envs\": 4, # number of parallel environments to use for training\n",
    "    \"checkpoint\": None, # path to a checkpoint to load from\n",
    "    \"checkpoint_freq\": 10000, # save a model checkpoint every _ steps\n",
    "    \"eval_freq\": 5000, # evaluate the model every _ steps\n",
    "    \"n_eval_episodes\": 10, # number of episodes to evaluate the model on\n",
    "    \"n_train_timesteps\": int(1e6), # total number of training steps\n",
    "    \"verbose_training\": True,\n",
    "    # RL PARAMETERS (all set to defaults right now, except for seed)\n",
    "    \"policy_args\": {\n",
    "        \"net_arch\": [64, 64],\n",
    "        \"activation_fn\": nn.ReLU,\n",
    "    },\n",
    "    \"algo_kwargs\": {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"buffer_size\": int(1e6),\n",
    "        \"learning_starts\": 1000, # how many steps of the model to collect transitions for before learning starts\n",
    "        \"batch_size\": 32,\n",
    "        \"tau\": 1.0, # the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
    "        \"gamma\": 0.99,\n",
    "        \"train_freq\": (4, 'step'), # Update the model every ``train_freq`` steps.\n",
    "        \"gradient_steps\": 1, # How many gradient steps to do after each rollout\n",
    "        \"target_update_interval\": int(1e4), # update the target network every ``target_update_interval`` environment steps.\n",
    "        #\"exploration_fraction\": 0.1, # fraction of entire training period over which the exploration rate is reduced\n",
    "        #\"exploration_initial_eps\": 1.0, # initial value of random action probability\n",
    "        #\"exploration_final_eps\": 0.05, # final value of random action probability\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39f990362fc45d9ae2a44eaf3ce1bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">set_curr_trend\n",
       "</pre>\n"
      ],
      "text/plain": [
       "set_curr_trend\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">in set_curr_trend\n",
       "</pre>\n"
      ],
      "text/plain": [
       "in set_curr_trend\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">start rescore\n",
       "</pre>\n"
      ],
      "text/plain": [
       "start rescore\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">heapify\n",
       "</pre>\n"
      ],
      "text/plain": [
       "heapify\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">heapify done\n",
       "</pre>\n"
      ],
      "text/plain": [
       "heapify done\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/094368j15ndfwyc6gh_z4pc80000gn/T/ipykernel_93913/2468420837.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mser_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mser_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./train_logs/ser\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mser_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"algo_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replay_buffer_class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSERReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3l/094368j15ndfwyc6gh_z4pc80000gn/T/ipykernel_93913/2001439112.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(env_spawner, eval_env_spawner, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     model.learn(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         callback=[\n\u001b[1;32m     64\u001b[0m             \u001b[0meval_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mtb_log_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAC\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ) -> SelfSAC:\n\u001b[0;32m--> 308\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You must set the environment before calling learn()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainFreq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# check done in _setup_learn()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0maction_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;31m# Only stop training if return value is False, not when it is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mRolloutReturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_collected_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_collected_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;31m# Retrieve reward and episode length if using Monitor wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Return False (stop training) if at least one callback returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Return False (stop training) if at least one callback returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# Reset success rate buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_success_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             episode_rewards, episode_lengths = evaluate_policy(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mepisode_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_starts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mnew_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mcurrent_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_monitor.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_returns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mnew_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# convert to SB3 VecEnv api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3l/094368j15ndfwyc6gh_z4pc80000gn/T/ipykernel_93913/1668605595.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcurrent_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Loop through each ticker and perform the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Get the current price of the stock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             current_prices[ticker] = self.stock_data[ticker].iloc[self.current_step][\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             ]\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# get the action for the current ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3998\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4001\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4002\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4003\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4004\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   6251\u001b[0m                \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently\u001b[0m \u001b[0mconsidered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6252\u001b[0m                \u001b[0mstable\u001b[0m \u001b[0macross\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mreleases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6253\u001b[0m         \"\"\"\n\u001b[1;32m   6254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6255\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6256\u001b[0m                 \u001b[0;31m# We want attrs propagation to have minimal performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6257\u001b[0m                 \u001b[0;31m# impact if attrs are not used; i.e. attrs is an empty dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6258\u001b[0m                 \u001b[0;31m# One could make the deepcopy unconditionally, but a deepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/licrom/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m    366\u001b[0m         \u001b[0mDictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ser_config = base_config.copy()\n",
    "ser_config[\"log_dir\"] = \"./train_logs/ser\"\n",
    "ser_config[\"algo_kwargs\"][\"replay_buffer_class\"] = SERReplayBuffer\n",
    "\n",
    "ser_config[\"priority_queue_size\"] = 5000\n",
    "ser_config[\"priority_queue_percent\"] = 0.5\n",
    "\n",
    "\n",
    "ser_config[\"algo_kwargs\"][\"replay_buffer_kwargs\"] = {\n",
    "    #\"strategy\": ser_config[\"strategy\"],\n",
    "    \"priority_queue_size\": ser_config[\"priority_queue_size\"],\n",
    "    \"priority_queue_percent\" : ser_config[\"priority_queue_percent\"]\n",
    "}\n",
    "\n",
    "train(make_env, make_eval_env, ser_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluations(log_dir):\n",
    "    evaluations = np.load(os.path.join(log_dir, \"evaluations.npz\"))\n",
    "    timesteps = evaluations[\"timesteps\"]\n",
    "    results_all = evaluations[\"results\"]  # shape: (eval_rounds, episodes_per_eval)\n",
    "    \n",
    "    mean_returns = np.mean(results_all, axis=1)\n",
    "    std_returns = np.std(results_all, axis=1)\n",
    "    num_episodes_averaged = results_all.shape[1]\n",
    "    print(\"Evaluations: min: {}, max: {}, std: {}\".format(\n",
    "        np.min(results_all), np.max(results_all), np.std(results_all)))\n",
    "        \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(timesteps, mean_returns, label='Mean Return')\n",
    "    plt.fill_between(timesteps, mean_returns - std_returns, mean_returns + std_returns,\n",
    "                     alpha=0.3, label='Std Dev')\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Returns (averaged over {} episodes)'.format(num_episodes_averaged))\n",
    "    plt.title('Evaluation Returns')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.savefig(os.path.join(log_dir, \"evaluations.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79331b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluations(ser_config[\"log_dir\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licrom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
