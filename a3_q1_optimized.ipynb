{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import StepAPICompatibility\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "State space: Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n"
     ]
    }
   ],
   "source": [
    "acrobot_env_name = 'Acrobot-v1'\n",
    "acrobot_env = gym.make(acrobot_env_name)\n",
    "# acrobot_env = StepAPICompatibility(acrobot_env)\n",
    "print(\"Action space:\", acrobot_env.action_space)\n",
    "print(\"State space:\", acrobot_env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(7)\n",
      "State space: Box(0, 255, (128,), uint8)\n"
     ]
    }
   ],
   "source": [
    "assault_env_name = 'ALE/Assault-ram-v5'\n",
    "assault_env = gym.make(assault_env_name)\n",
    "# assault_env = StepAPICompatibility(assault_env)\n",
    "print(\"Action space:\", assault_env.action_space)\n",
    "print(\"State space:\", assault_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temp):\n",
    "    z = np.exp(x / temp - np.max(x / temp))\n",
    "    return z / np.sum(z)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "        self.mlp.apply(self.init_weights)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.uniform_(m.weight, -0.001, 0.001)\n",
    "            nn.init.uniform_(m.bias, -0.001, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class DeepValueLearning:\n",
    "    def __init__(self, env, step_size, epsilon, algorithm, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.algorithm = algorithm\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.Q = QNetwork(self.state_dim, self.n_actions, self.device)\n",
    "        self.optimizer = optim.SGD(self.Q.parameters(), lr=self.step_size)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, s):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            state_input = torch.as_tensor(s).float().unsqueeze(0).to(self.device)\n",
    "            return torch.argmax(self.Q(state_input))\n",
    "\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "    ):\n",
    "        q_val_batch = self.Q(state_batch)\n",
    "        q_val_batch = q_val_batch.gather(1, action_batch.unsqueeze(1))\n",
    "        q_val_batch = q_val_batch.squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            done_batch = 1.0 - done_batch\n",
    "            next_q_val = self.Q(next_state_batch)\n",
    "            greedy_next_q_val, _ = next_q_val.max(dim=1)\n",
    "            if self.algorithm == \"Q-Learning\":\n",
    "                target_batch = (\n",
    "                    reward_batch + done_batch * self.gamma * greedy_next_q_val\n",
    "                )\n",
    "            else:\n",
    "                random_next_q_val = next_q_val.mean(dim=1)\n",
    "                exp_next_q_val = (\n",
    "                    self.epsilon * random_next_q_val\n",
    "                    + (1 - self.epsilon) * greedy_next_q_val\n",
    "                )\n",
    "                target_batch = reward_batch + done_batch * self.gamma * exp_next_q_val\n",
    "\n",
    "        # Compute loss and update weights\n",
    "        loss = self.loss_fn(q_val_batch, target_batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.device = device\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(epsilon, step_size, seed, env, algorithm, use_buffer, env_name):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    agent = DeepValueLearning(env, step_size, epsilon, algorithm)\n",
    "    if use_buffer:\n",
    "        replay_buffer = ReplayBuffer(1_000_000, agent.device)\n",
    "        replay_minibatch_size = 16\n",
    "        \n",
    "    max_steps_per_episode = 300\n",
    "    \n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for _ in tqdm.tqdm(range(1000)):\n",
    "        state, _ = env.reset()\n",
    "        if env_name == \"ALE/Assault-ram-v5\":\n",
    "            state = state/255\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        n_steps = 0\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "             \n",
    "            if env_name == \"ALE/Assault-ram-v5\":\n",
    "                next_state = next_state/255\n",
    "            done = done or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if use_buffer: replay_buffer.push([torch.as_tensor(state).to(agent.device),\n",
    "                                               torch.as_tensor(action).to(agent.device),\n",
    "                                               torch.as_tensor(reward).to(agent.device),\n",
    "                                               torch.as_tensor(next_state).to(agent.device),\n",
    "                                               torch.as_tensor(done).float().to(agent.device)])\n",
    "            \n",
    "            if use_buffer:\n",
    "                if n_steps % replay_minibatch_size and len(replay_buffer) > replay_minibatch_size:\n",
    "                    transitions = replay_buffer.sample(replay_minibatch_size)\n",
    "                    state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*transitions)\n",
    "                    state_batch = torch.stack(state_batch)\n",
    "                    action_batch = torch.stack(action_batch)\n",
    "                    reward_batch = torch.stack(reward_batch)\n",
    "                    next_state_batch = torch.stack(next_state_batch)\n",
    "                    done_batch = torch.stack(done_batch)\n",
    "                    agent.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "            else:\n",
    "                agent.update(torch.as_tensor(state).to(agent.device).unsqueeze(0),\n",
    "                             torch.as_tensor(action).to(agent.device).unsqueeze(0),\n",
    "                             torch.as_tensor(reward).to(agent.device).unsqueeze(0),\n",
    "                             torch.as_tensor(next_state).to(agent.device).unsqueeze(0),\n",
    "                             torch.as_tensor(done).float().to(agent.device).unsqueeze(0))\n",
    "                \n",
    "            state = next_state\n",
    "            n_steps += 1\n",
    "            if n_steps >= max_steps_per_episode:\n",
    "                done = True\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "    return episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial # 6 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:19<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 6/720 trials\n",
      "Starting trial # 7 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:09<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 7/720 trials\n",
      "Starting trial # 8 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:47<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 8/720 trials\n",
      "Starting trial # 9 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:55<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 9/720 trials\n",
      "Starting trial # 10 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:52<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10/720 trials\n",
      "Starting trial # 11 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:24<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 11/720 trials\n",
      "Starting trial # 12 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:23<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 12/720 trials\n",
      "Starting trial # 13 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:16<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 13/720 trials\n",
      "Starting trial # 14 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:39<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 14/720 trials\n",
      "Starting trial # 15 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 15/720 trials\n",
      "Starting trial # 16 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 253/1000 [02:09<07:16,  1.71it/s]"
     ]
    }
   ],
   "source": [
    "epsilons = [0.01, 0.1, 0.5]\n",
    "step_sizes = [1/4, 1/8, 1/16]\n",
    "seeds = range(10)\n",
    "envs = [acrobot_env, assault_env]\n",
    "algorithms = [\"Expected-SARSA\", \"Q-Learning\"]\n",
    "\n",
    "total_trials = len(envs) * len(seeds) * len(epsilons) * len(step_sizes) * len(algorithms) * 2\n",
    "\n",
    "\n",
    "# load pickle file for results\n",
    "# check if file exists\n",
    "try:\n",
    "    with open('results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "        trials_completed = len(results)\n",
    "except:\n",
    "    results = {}\n",
    "    trials_completed = 0\n",
    "\n",
    "for env in envs:\n",
    "    env_name = env.env.spec.id\n",
    "    for epsilon in epsilons:\n",
    "        for step_size in step_sizes:\n",
    "            for algorithm in algorithms:\n",
    "                for use_buffer in [True, False]:\n",
    "                    for seed in seeds:\n",
    "                        print(\"Starting trial #\", trials_completed + 1, \"/\", total_trials)\n",
    "                        \n",
    "                        episode_rewards = run_trial(epsilon, step_size, seed, env, algorithm, use_buffer, env_name)\n",
    "                        results[(env_name, seed, epsilon, step_size, algorithm, use_buffer)] = episode_rewards\n",
    "                        with open('results.pkl', 'wb') as f:\n",
    "                            pickle.dump(results, f)\n",
    "                        trials_completed += 1\n",
    "                        print(f\"Completed {trials_completed}/{total_trials} trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
