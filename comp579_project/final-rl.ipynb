{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5336fafe-5dcd-415b-9146-a69855b2d3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T13:59:51.718824Z",
     "iopub.status.busy": "2025-04-23T13:59:51.718572Z",
     "iopub.status.idle": "2025-04-23T14:00:10.283723Z",
     "shell.execute_reply": "2025-04-23T14:00:10.282914Z",
     "shell.execute_reply.started": "2025-04-23T13:59:51.718807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install finrl\\n!pip install alpaca_trade_api\\n!pip install exchange_calendars\\n!pip install stockstats\\n!pip install wrds\\n!pip install yfinance'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install finrl\n",
    "!pip install alpaca_trade_api\n",
    "!pip install exchange_calendars\n",
    "!pip install stockstats\n",
    "!pip install wrds\n",
    "!pip install yfinance\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828fc3dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:00:15.533499Z",
     "iopub.status.busy": "2025-04-23T14:00:15.533178Z",
     "iopub.status.idle": "2025-04-23T14:00:15.540587Z",
     "shell.execute_reply": "2025-04-23T14:00:15.539619Z",
     "shell.execute_reply.started": "2025-04-23T14:00:15.533473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import heapq\n",
    "import random\n",
    "import torch\n",
    "from scipy.spatial import KDTree\n",
    "from torch import nn\n",
    "import itertools\n",
    "import IPython.display\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "from stable_baselines3 import DQN, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.buffers import *\n",
    "from stable_baselines3.common.logger import configure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184d9fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:01:08.376982Z",
     "iopub.status.busy": "2025-04-23T14:01:08.376276Z",
     "iopub.status.idle": "2025-04-23T14:01:09.723375Z",
     "shell.execute_reply": "2025-04-23T14:01:09.722723Z",
     "shell.execute_reply.started": "2025-04-23T14:01:08.376951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3428, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\john\\AppData\\Local\\Temp\\ipykernel_58940\\2786363171.py:32: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  data_df = data_df.drop(columns=[\"adjcp\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>1.438994</td>\n",
       "      <td>1.440196</td>\n",
       "      <td>1.432706</td>\n",
       "      <td>1.432706</td>\n",
       "      <td>0</td>\n",
       "      <td>EURUSD=X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1.442398</td>\n",
       "      <td>1.445191</td>\n",
       "      <td>1.426208</td>\n",
       "      <td>1.431004</td>\n",
       "      <td>0</td>\n",
       "      <td>EURUSD=X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1.436596</td>\n",
       "      <td>1.448310</td>\n",
       "      <td>1.435194</td>\n",
       "      <td>1.442710</td>\n",
       "      <td>0</td>\n",
       "      <td>EURUSD=X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1.440403</td>\n",
       "      <td>1.443460</td>\n",
       "      <td>1.429123</td>\n",
       "      <td>1.436596</td>\n",
       "      <td>0</td>\n",
       "      <td>EURUSD=X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1.431803</td>\n",
       "      <td>1.444481</td>\n",
       "      <td>1.430206</td>\n",
       "      <td>1.440300</td>\n",
       "      <td>0</td>\n",
       "      <td>EURUSD=X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        date     close      high       low      open  volume       tic\n",
       "0      2010-01-01  1.438994  1.440196  1.432706  1.432706       0  EURUSD=X\n",
       "1      2010-01-04  1.442398  1.445191  1.426208  1.431004       0  EURUSD=X\n",
       "2      2010-01-05  1.436596  1.448310  1.435194  1.442710       0  EURUSD=X\n",
       "3      2010-01-06  1.440403  1.443460  1.429123  1.436596       0  EURUSD=X\n",
       "4      2010-01-07  1.431803  1.444481  1.430206  1.440300       0  EURUSD=X"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the EURUSD=X data\n",
    "\n",
    "TRAIN_START_DATE = \"2010-01-01\"\n",
    "TRAIN_END_DATE = \"2021-10-01\"\n",
    "TEST_START_DATE = \"2021-10-01\"\n",
    "TEST_END_DATE = \"2023-03-01\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "temp_df = yf.download(\n",
    "    \"EURUSD=X\", start=TRAIN_START_DATE, end=TEST_END_DATE, auto_adjust=False\n",
    ")\n",
    "temp_df[\"tic\"] = \"EURUSD=X\"\n",
    "dfs.append(temp_df)\n",
    "\n",
    "data_df = pd.concat(dfs)\n",
    "data_df = data_df.reset_index()\n",
    "\n",
    "\n",
    "data_df = data_df.rename(\n",
    "    columns={\n",
    "        \"Date\": \"date\",\n",
    "        \"Open\": \"open\",\n",
    "        \"High\": \"high\",\n",
    "        \"Low\": \"low\",\n",
    "        \"Close\": \"close\",\n",
    "        \"Adj Close\": \"adjcp\",\n",
    "        \"Volume\": \"volume\",\n",
    "    }\n",
    ")\n",
    "\n",
    "data_df = data_df.drop(columns=[\"adjcp\"])\n",
    "data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# drop missing data\n",
    "data_df = data_df.dropna()\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "print(\"Shape of DataFrame: \", data_df.shape)\n",
    "\n",
    "data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
    "data_df.columns = data_df.columns.get_level_values(0)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c9c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n",
      "         date     close      high       low      open  volume       tic  \\\n",
      "0  2010-01-01  1.438994  1.440196  1.432706  1.432706       0  EURUSD=X   \n",
      "1  2010-01-04  1.442398  1.445191  1.426208  1.431004       0  EURUSD=X   \n",
      "2  2010-01-05  1.436596  1.448310  1.435194  1.442710       0  EURUSD=X   \n",
      "3  2010-01-06  1.440403  1.443460  1.429123  1.436596       0  EURUSD=X   \n",
      "4  2010-01-07  1.431803  1.444481  1.430206  1.440300       0  EURUSD=X   \n",
      "\n",
      "       macd      rsi_30      cci_30       dx_30  turbulence  \n",
      "0  0.000000  100.000000   66.666667  100.000000         0.0  \n",
      "1  0.000076  100.000000   66.666667  100.000000         0.0  \n",
      "2 -0.000083   36.189879  100.000000   33.643671         0.0  \n",
      "3 -0.000015   55.476684  -42.150727   60.221140         0.0  \n",
      "4 -0.000321   32.513547 -140.430918   49.776959         0.0  \n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]\n",
    "\n",
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    use_vix=False,\n",
    "    use_turbulence=True,\n",
    "    user_defined_feature=False,\n",
    ")\n",
    "\n",
    "processed = fe.preprocess_data(data_df)\n",
    "print(processed.head())\n",
    "\n",
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(\n",
    "    pd.date_range(processed[\"date\"].min(), processed[\"date\"].max()).astype(str)\n",
    ")\n",
    "combination = list(itertools.product(list_date, list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination, columns=[\"date\", \"tic\"]).merge(\n",
    "    processed, on=[\"date\", \"tic\"], how=\"left\"\n",
    ")\n",
    "processed_full = processed_full[processed_full[\"date\"].isin(processed[\"date\"])]\n",
    "processed_full = processed_full.sort_values([\"date\", \"tic\"])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79cc01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "368\n",
      "         date       tic     close      high       low      open  volume  \\\n",
      "0  2010-01-01  EURUSD=X  1.438994  1.440196  1.432706  1.432706     0.0   \n",
      "1  2010-01-04  EURUSD=X  1.442398  1.445191  1.426208  1.431004     0.0   \n",
      "2  2010-01-05  EURUSD=X  1.436596  1.448310  1.435194  1.442710     0.0   \n",
      "3  2010-01-06  EURUSD=X  1.440403  1.443460  1.429123  1.436596     0.0   \n",
      "4  2010-01-07  EURUSD=X  1.431803  1.444481  1.430206  1.440300     0.0   \n",
      "\n",
      "       macd      rsi_30      cci_30       dx_30  turbulence  \n",
      "0  0.000000  100.000000   66.666667  100.000000         0.0  \n",
      "1  0.000076  100.000000   66.666667  100.000000         0.0  \n",
      "2 -0.000083   36.189879  100.000000   33.643671         0.0  \n",
      "3 -0.000015   55.476684  -42.150727   60.221140         0.0  \n",
      "4 -0.000321   32.513547 -140.430918   49.776959         0.0  \n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))\n",
    "\n",
    "# Save to file\n",
    "# train.to_csv('train_data.csv')\n",
    "# trade.to_csv('trade_data.csv')\n",
    "\n",
    "# train = train.set_index(\"date\", drop=False)\n",
    "# train.index.names = [\"\"]\n",
    "print(train.head())\n",
    "\n",
    "# trade = trade.set_index(\"date\", drop=False)\n",
    "# trade.index.names = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f54202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:04:18.961213Z",
     "iopub.status.busy": "2025-04-23T14:04:18.960490Z",
     "iopub.status.idle": "2025-04-23T14:04:18.965769Z",
     "shell.execute_reply": "2025-04-23T14:04:18.965033Z",
     "shell.execute_reply.started": "2025-04-23T14:04:18.961192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 7\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77547adf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:05:25.240248Z",
     "iopub.status.busy": "2025-04-23T14:05:25.239490Z",
     "iopub.status.idle": "2025-04-23T14:05:25.248865Z",
     "shell.execute_reply": "2025-04-23T14:05:25.248112Z",
     "shell.execute_reply.started": "2025-04-23T14:05:25.240227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date       tic     close      high       low      open  volume  \\\n",
      "0  2010-01-01  EURUSD=X  1.438994  1.440196  1.432706  1.432706     0.0   \n",
      "1  2010-01-04  EURUSD=X  1.442398  1.445191  1.426208  1.431004     0.0   \n",
      "2  2010-01-05  EURUSD=X  1.436596  1.448310  1.435194  1.442710     0.0   \n",
      "3  2010-01-06  EURUSD=X  1.440403  1.443460  1.429123  1.436596     0.0   \n",
      "4  2010-01-07  EURUSD=X  1.431803  1.444481  1.430206  1.440300     0.0   \n",
      "\n",
      "       macd      rsi_30      cci_30       dx_30  turbulence  \n",
      "0  0.000000  100.000000   66.666667  100.000000         0.0  \n",
      "1  0.000076  100.000000   66.666667  100.000000         0.0  \n",
      "2 -0.000083   36.189879  100.000000   33.643671         0.0  \n",
      "3 -0.000015   55.476684  -42.150727   60.221140         0.0  \n",
      "4 -0.000321   32.513547 -140.430918   49.776959         0.0  \n",
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "try:\n",
    "    e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    print(type(env_train))\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. Check if 'train' DataFrame is correctly initialized and not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd93fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:05:28.579082Z",
     "iopub.status.busy": "2025-04-23T14:05:28.578366Z",
     "iopub.status.idle": "2025-04-23T14:05:28.625958Z",
     "shell.execute_reply": "2025-04-23T14:05:28.625185Z",
     "shell.execute_reply.started": "2025-04-23T14:05:28.579062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: still need to fix/add \"reward\" strategy; also check coverage\n",
    "\n",
    "class SERReplayBuffer(BaseBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        strategy: str,\n",
    "        priority_queue_size: int,\n",
    "        priority_queue_percent: float, # 10% use --> 0.1\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "        optimize_memory_usage: bool = False,\n",
    "        handle_timeout_termination: bool = True,\n",
    "    ):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n",
    "\n",
    "        # Adjust buffer size\n",
    "        self.buffer_size = max(buffer_size // n_envs, 1)\n",
    "\n",
    "        # Check that the replay buffer can fit into the memory\n",
    "        if psutil is not None:\n",
    "            mem_available = psutil.virtual_memory().available\n",
    "\n",
    "        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n",
    "        if optimize_memory_usage and handle_timeout_termination:\n",
    "            raise ValueError(\n",
    "                \"ReplayBuffer does not support optimize_memory_usage = True \"\n",
    "                \"and handle_timeout_termination = True simultaneously.\"\n",
    "            )\n",
    "        self.optimize_memory_usage = optimize_memory_usage\n",
    "\n",
    "        self.observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        if not optimize_memory_usage:\n",
    "            # When optimizing memory, `observations` contains also the next observation\n",
    "            self.next_observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        self.actions = np.zeros(\n",
    "            (self.buffer_size, self.n_envs, self.action_dim), dtype=self._maybe_cast_dtype(action_space.dtype)\n",
    "        )\n",
    "\n",
    "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        # Handle timeouts termination properly if needed\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "        self.handle_timeout_termination = handle_timeout_termination\n",
    "        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "\n",
    "        if psutil is not None:\n",
    "            total_memory_usage: float = (\n",
    "                self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "            )\n",
    "\n",
    "            if not optimize_memory_usage:\n",
    "                total_memory_usage += self.next_observations.nbytes\n",
    "\n",
    "            if total_memory_usage > mem_available:\n",
    "                # Convert to GB\n",
    "                total_memory_usage /= 1e9\n",
    "                mem_available /= 1e9\n",
    "                warnings.warn(\n",
    "                    \"This system does not have apparently enough memory to store the complete \"\n",
    "                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n",
    "                )\n",
    "        \n",
    "        self.strategy = strategy  \n",
    "        self.gamma = 0.99\n",
    "        self.dist_threshold = 0.5\n",
    "        self.priority_queue_size= priority_queue_size\n",
    "        self.long_term_memory =[]  # (score, (obs, next_obs, action, reward, done))\n",
    "        self.priority_queue_percent = priority_queue_percent\n",
    "        self.q_net = None\n",
    "        self.q_net_target = None\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        infos: list[dict[str, Any]],\n",
    "    ) -> None:\n",
    "        # Reshape needed when using multiple envs with discrete observations\n",
    "        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n",
    "        if isinstance(self.observation_space, spaces.Discrete):\n",
    "            obs = obs.reshape((self.n_envs, *self.obs_shape))\n",
    "            next_obs = next_obs.reshape((self.n_envs, *self.obs_shape))\n",
    "\n",
    "        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n",
    "        action = action.reshape((self.n_envs, self.action_dim))\n",
    "\n",
    "        # Copy to avoid modification by reference\n",
    "        self.observations[self.pos] = np.array(obs)\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs)\n",
    "        else:\n",
    "            self.next_observations[self.pos] = np.array(next_obs)\n",
    "\n",
    "        self.actions[self.pos] = np.array(action)\n",
    "        self.rewards[self.pos] = np.array(reward)\n",
    "        self.dones[self.pos] = np.array(done)\n",
    "\n",
    "        if self.handle_timeout_termination:\n",
    "            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n",
    "\n",
    "        self.pos += 1\n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "            self.pos = 0\n",
    "\n",
    "        # compute score using given strategy and push to priority queue\n",
    "        # will get rid of the lowest score among the ones stored in long term mem if we exceed memory limit\n",
    "        \n",
    "        idx = (self.pos - 1) % self.buffer_size\n",
    "        for env_idx in range(self.n_envs):\n",
    "            score = self.compute_score(obs[env_idx], next_obs[env_idx], action[env_idx], reward[env_idx], done[env_idx])\n",
    "            heapq.heappush(self.long_term_memory, (score, idx, env_idx))\n",
    "\n",
    "        if len(self.long_term_memory) > self.priority_queue_size:\n",
    "            heapq.heappop(self.long_term_memory)\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None):\n",
    "        #print(\"sampling\")\n",
    "        long_size=int(batch_size*self.priority_queue_percent)\n",
    "        fifo_size=batch_size-long_size\n",
    "        \n",
    "        # sample from FIFO\n",
    "        if not self.optimize_memory_usage:\n",
    "            return super().sample(batch_size=batch_size, env=env)\n",
    "        # Do not sample the element with index `self.pos` as the transitions is invalid\n",
    "        # (we use only one array to store `obs` and `next_obs`)\n",
    "        if self.full:\n",
    "            fifo_inds = (np.random.randint(1, self.buffer_size, size=fifo_size) + self.pos) % self.buffer_size\n",
    "        else:\n",
    "            fifo_inds = np.random.randint(0, self.pos, size=fifo_size)\n",
    "\n",
    "        fifo_samples = self._get_samples(fifo_inds, env=env)\n",
    "\n",
    "        # sample from long term memoru\n",
    "        if len(self.long_term_memory) >= long_size:\n",
    "            sampled_long_mem = random.sample(self.long_term_memory, long_size)\n",
    "        else:\n",
    "            sampled_long_mem = self.long_term_memory\n",
    "\n",
    "        buffer_idxs = [idx for _, idx, env in sampled_long_mem]\n",
    "        env_idxs = [env for _, idx, env in sampled_long_mem]\n",
    "\n",
    "        long_obs = self.observations[buffer_idxs, env_idxs]\n",
    "        if self.optimize_memory_usage:\n",
    "            long_next_obs = self.observations[(np.array(buffer_idxs) + 1) % self.buffer_size, env_idxs]\n",
    "        else:\n",
    "            long_next_obs = self.next_observations[buffer_idxs, env_idxs]\n",
    "\n",
    "        long_actions = self.actions[buffer_idxs, env_idxs]\n",
    "        long_rewards = self.rewards[buffer_idxs, env_idxs].reshape(-1, 1)\n",
    "        long_dones = (self.dones[buffer_idxs, env_idxs] * (1 - self.timeouts[buffer_idxs, env_idxs])).reshape(-1, 1)\n",
    "\n",
    "        long_obs = self.to_torch(self._normalize_obs(long_obs, env))\n",
    "        long_next_obs = self.to_torch(self._normalize_obs(long_next_obs, env))\n",
    "        long_actions = self.to_torch(long_actions)\n",
    "        long_rewards = self.to_torch(self._normalize_reward(long_rewards, env))\n",
    "        long_dones = self.to_torch(long_dones)\n",
    "\n",
    "        # combine FIFO + long-term mem\n",
    "        obs = torch.cat([fifo_samples.observations, long_obs], dim=0)\n",
    "        next_obs = torch.cat([fifo_samples.next_observations, long_next_obs], dim=0)\n",
    "        actions = torch.cat([fifo_samples.actions, long_actions], dim=0)\n",
    "        rewards = torch.cat([fifo_samples.rewards, long_rewards], dim=0)\n",
    "        dones = torch.cat([fifo_samples.dones, long_dones], dim=0)\n",
    "\n",
    "        return ReplayBufferSamples(obs, actions, next_obs, dones, rewards)\n",
    "\n",
    "    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> ReplayBufferSamples:\n",
    "        # Sample randomly the env idx\n",
    "        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            next_obs = self._normalize_obs(self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :], env)\n",
    "        else:\n",
    "            next_obs = self._normalize_obs(self.next_observations[batch_inds, env_indices, :], env)\n",
    "\n",
    "        data = (\n",
    "            self._normalize_obs(self.observations[batch_inds, env_indices, :], env),\n",
    "            self.actions[batch_inds, env_indices, :],\n",
    "            next_obs,\n",
    "            # Only use dones that are not due to timeouts\n",
    "            # deactivated by default (timeouts is initialized as an array of False)\n",
    "            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n",
    "            self._normalize_reward(self.rewards[batch_inds, env_indices].reshape(-1, 1), env),\n",
    "        )\n",
    "        #print(\"sampling done\")\n",
    "        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "\n",
    "    def compute_score(self, obs, next_obs, action, reward, done):\n",
    "        if self.strategy == \"reward\":\n",
    "            return float(abs(reward)) # TODO: need to implement this one\n",
    "        elif self.strategy == \"distribution\":\n",
    "            return float(np.random.normal()) \n",
    "        \n",
    "        elif self.strategy== \"surprise\":\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = self.to_torch(obs).unsqueeze(0)\n",
    "                next_obs_tensor = self.to_torch(next_obs).unsqueeze(0)  \n",
    "                action_tensor = self.to_torch(action).long().unsqueeze(0)  \n",
    "                reward_tensor = self.to_torch(reward).unsqueeze(0)\n",
    "                done_tensor = self.to_torch(done).unsqueeze(0).float()\n",
    "\n",
    "                q_values = self.q_net(obs_tensor)\n",
    "                q_sa = q_values.gather(1, action_tensor)\n",
    "\n",
    "                next_q_values = self.q_net_target(next_obs_tensor)\n",
    "                max_q_next = next_q_values.max(1, keepdim=True).values\n",
    "\n",
    "                td_target = reward_tensor + self.gamma * (1.0 - done_tensor) * max_q_next\n",
    "                td_error = torch.abs(td_target - q_sa)\n",
    "                return float(td_error.item())\n",
    "            \n",
    "        elif self.strategy == \"coverage\": \n",
    "           norm_obs = self._normalize_obs(obs, env=None).flatten()\n",
    "           # build KD tree\n",
    "           all = []\n",
    "           for _, idx, env in self.long_term_memory:\n",
    "               exist_obs = self._normalize_obs(self.observations[idx, env], env=None).flatten()\n",
    "               all.append(exist_obs)\n",
    "\n",
    "           if len(all) == 0:\n",
    "               return 0\n",
    "           tree= KDTree(np.stack(all))\n",
    "           neighbors = tree.query_ball_point(norm_obs, r=self.dist_threshold)\n",
    "           count= len(neighbors)\n",
    "           return -count\n",
    "        return 0\n",
    "    \n",
    "    def set_q_nets(self, q_net, q_net_target):\n",
    "        self.q_net = q_net\n",
    "        self.q_net_target = q_net_target\n",
    "    \n",
    "    @staticmethod\n",
    "    def _maybe_cast_dtype(dtype: np.typing.DTypeLike) -> np.typing.DTypeLike:\n",
    "        \"\"\"\n",
    "        Cast `np.float64` action datatype to `np.float32`,\n",
    "        keep the others dtype unchanged.\n",
    "        See GH#1572 for more information.\n",
    "\n",
    "        :param dtype: The original action space dtype\n",
    "        :return: ``np.float32`` if the dtype was float64,\n",
    "            the original dtype otherwise.\n",
    "        \"\"\"\n",
    "        if dtype == np.float64:\n",
    "            return np.float32\n",
    "        return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128592fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:05:33.761978Z",
     "iopub.status.busy": "2025-04-23T14:05:33.761213Z",
     "iopub.status.idle": "2025-04-23T14:05:33.767382Z",
     "shell.execute_reply": "2025-04-23T14:05:33.766790Z",
     "shell.execute_reply.started": "2025-04-23T14:05:33.761945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    # TRAINING PARAMETERS\n",
    "    \"log_dir\": \"./train_logs/base\",\n",
    "    \"n_envs\": 32, # number of parallel environments to use for training\n",
    "    \"checkpoint\": None, # path to a checkpoint to load from\n",
    "    \"checkpoint_freq\": 10000, # save a model checkpoint every _ steps\n",
    "    \"eval_freq\": 5000, # evaluate the model every _ steps\n",
    "    \"n_eval_episodes\": 10, # number of episodes to evaluate the model on\n",
    "    \"n_train_timesteps\": int(1e6), # total number of training steps\n",
    "    \"verbose_training\": True,\n",
    "    # RL PARAMETERS (all set to defaults right now, except for seed)\n",
    "    \"policy_args\": {\n",
    "        \"net_arch\": [64, 64],\n",
    "        \"activation_fn\": nn.ReLU,\n",
    "    },\n",
    "    \"algo_kwargs\": {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"buffer_size\": int(1e6),\n",
    "        \"learning_starts\": 100, # how many steps of the model to collect transitions for before learning starts\n",
    "        \"batch_size\": 32,\n",
    "        \"tau\": 1.0, # the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
    "        \"gamma\": 0.99,\n",
    "        \"train_freq\": (4, 'step'), # Update the model every ``train_freq`` steps.\n",
    "        \"gradient_steps\": 1, # How many gradient steps to do after each rollout\n",
    "        \"target_update_interval\": int(1e4), # update the target network every ``target_update_interval`` environment steps.\n",
    "        #\"exploration_fraction\": 0.1, # fraction of entire training period over which the exploration rate is reduced\n",
    "        #\"exploration_initial_eps\": 1.0, # initial value of random action probability\n",
    "        #\"exploration_final_eps\": 0.05, # final value of random action probability\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d05bdee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:05:36.198378Z",
     "iopub.status.busy": "2025-04-23T14:05:36.198045Z",
     "iopub.status.idle": "2025-04-23T14:05:39.422244Z",
     "shell.execute_reply": "2025-04-23T14:05:39.421605Z",
     "shell.execute_reply.started": "2025-04-23T14:05:36.198359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "\n",
    "# set up logger\n",
    "tmp_path = RESULTS_DIR + '/sac'\n",
    "new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\"])\n",
    "# Set new logger\n",
    "model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d049d2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:05:39.424382Z",
     "iopub.status.busy": "2025-04-23T14:05:39.423344Z",
     "iopub.status.idle": "2025-04-23T14:05:39.431168Z",
     "shell.execute_reply": "2025-04-23T14:05:39.430402Z",
     "shell.execute_reply.started": "2025-04-23T14:05:39.424362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_rl(agent, env_train, eval_env, algo, config):\n",
    "    print(\"Initializing...\")\n",
    "\n",
    "    log_dir = config[\"log_dir\"]\n",
    "    ckpt = config.get(\"checkpoint\", None)\n",
    "    algo_kwargs = config[\"algo_kwargs\"]\n",
    "    checkpoint_freq = config[\"checkpoint_freq\"]\n",
    "    eval_freq = config[\"eval_freq\"]\n",
    "    n_eval_episodes = config[\"n_eval_episodes\"]\n",
    "    n_train_timesteps = config[\"n_train_timesteps\"]\n",
    "    verbose_training = config[\"verbose_training\"]\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    model = agent.get_model(algo, model_kwargs=algo_kwargs)\n",
    "\n",
    "    if ckpt is not None:\n",
    "        model = model.load(ckpt, env=env_train)\n",
    "\n",
    "    # SER support if needed\n",
    "    # if hasattr(model, \"replay_buffer\") and hasattr(model.replay_buffer, \"set_q_nets\"):\n",
    "    #   model.replay_buffer.set_q_nets(model.q_net, model.q_net_target)\n",
    "\n",
    "    # Logger\n",
    "    new_logger = configure(log_dir, [\"stdout\", \"csv\"])\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=checkpoint_freq,\n",
    "        save_path=log_dir,\n",
    "        name_prefix=os.path.basename(log_dir),\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=eval_freq,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    try:\n",
    "        model.learn(\n",
    "            total_timesteps=n_train_timesteps,\n",
    "            callback=[checkpoint_callback, eval_callback],\n",
    "            log_interval=1 if verbose_training else None,\n",
    "            reset_num_timesteps=False,\n",
    "            progress_bar=True if verbose_training else False,\n",
    "        )\n",
    "    except BaseException as error:\n",
    "        print(f\"Error during training: {error}\")\n",
    "        raise\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1c4f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m                 latest_step = steps\n\u001b[32m     50\u001b[39m                 checkpoint_path = os.path.normpath(os.path.join(log_dir, filename))\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcheckpoint_path\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound latest checkpoint to load: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m     ser_config[\u001b[33m\"\u001b[39m\u001b[33mcheckpoint\u001b[39m\u001b[33m\"\u001b[39m] = checkpoint_path\n",
      "\u001b[31mNameError\u001b[39m: name 'checkpoint_path' is not defined"
     ]
    }
   ],
   "source": [
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "\n",
    "e_eval_gym = StockTradingEnv(df=trade, **env_kwargs)\n",
    "env_eval, _ = e_eval_gym.get_sb_env()\n",
    "\n",
    "agent = DRLAgent(env=env_train)\n",
    "\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "log_dir = \"./train_logs/base\"\n",
    "base_config = {\n",
    "    # TRAINING PARAMETERS\n",
    "    \"log_dir\": log_dir,\n",
    "    \"n_envs\": 32,  # number of parallel environments to use for training\n",
    "    \"checkpoint\": None,  # path to a checkpoint to load from\n",
    "    \"checkpoint_freq\": 10000,  # save a model checkpoint every _ steps\n",
    "    \"eval_freq\": 5000,  # evaluate the model every _ steps\n",
    "    \"n_eval_episodes\": 10,  # number of episodes to evaluate the model on\n",
    "    \"n_train_timesteps\": int(1e6),  # total number of training steps\n",
    "    \"verbose_training\": True,\n",
    "    # RL PARAMETERS (all set to defaults right now, except for seed)\n",
    "    \"policy_args\": {\n",
    "        \"net_arch\": [64, 64],\n",
    "        \"activation_fn\": nn.ReLU,\n",
    "    },\n",
    "    \"algo_kwargs\": SAC_PARAMS,\n",
    "}\n",
    "\n",
    "\n",
    "log_dir = \"./train_logs/ser\"\n",
    "ser_config = base_config.copy()\n",
    "ser_config[\"log_dir\"] = log_dir\n",
    "latest_step = -1\n",
    "checkpoint_path = None\n",
    "if os.path.exists(log_dir):\n",
    "    # Find files matching the checkpoint pattern (e.g., ser_10000_steps.zip)\n",
    "    pattern = re.compile(r\"ser_(\\d+)_steps\\.zip\")\n",
    "    for filename in os.listdir(log_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            steps = int(match.group(1))\n",
    "            if steps > latest_step:\n",
    "                latest_step = steps\n",
    "                checkpoint_path = os.path.normpath(os.path.join(log_dir, filename))\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"Found latest checkpoint to load: {checkpoint_path}\")\n",
    "    ser_config[\"checkpoint\"] = checkpoint_path\n",
    "else:\n",
    "    print(\"No previous checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "ser_config[\"algo_kwargs\"][\"replay_buffer_class\"] = SERReplayBuffer\n",
    "\n",
    "ser_config[\"strategy\"] = \"distribution\"\n",
    "ser_config[\"priority_queue_size\"] = 5000\n",
    "ser_config[\"priority_queue_percent\"] = 0.5\n",
    "\n",
    "\n",
    "ser_config[\"algo_kwargs\"][\"replay_buffer_kwargs\"] = {\n",
    "    \"strategy\": ser_config[\"strategy\"],\n",
    "    \"priority_queue_size\": ser_config[\"priority_queue_size\"],\n",
    "    \"priority_queue_percent\": ser_config[\"priority_queue_percent\"],\n",
    "}\n",
    "ser_config[\"algo_kwargs\"][\"device\"] = \"cpu\"\n",
    "\n",
    "\n",
    "IPython.display.clear_output(wait=True)\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "model = train_rl(agent, env_train, env_eval, \"sac\", ser_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=model, \n",
    "    environment = e_trade_gym) \n",
    "\n",
    "df_result_sac = (\n",
    "    df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    ")\n",
    "\n",
    "# we can add all the different SER strategies + FIFO here + maybe a baseline comparision?\n",
    "# i think theres either mean variance or DJIA \n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"sac\": df_result_sac[\"account_value\"] \n",
    "    }\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7231603,
     "sourceId": 11529734,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
