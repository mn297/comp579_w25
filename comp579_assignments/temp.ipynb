{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# COMP 579 - Assignment 3 - Winter 2025"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (if needed)\n",
                "!pip install gymnasium matplotlib torch numpy pandas seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import gymnasium as gym\n",
                "import time\n",
                "from collections import deque, namedtuple\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from IPython.display import clear_output\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "random.seed(42)\n",
                "np.random.seed(42)\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PART 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_environment(env_name=\"CartPole-v1\"):\n",
                "    \"\"\"Create and return the specified environment\"\"\"\n",
                "    env = gym.make(env_name)\n",
                "    print(f\"Environment: {env_name}\")\n",
                "    print(f\"Observation Space: {env.observation_space}\")\n",
                "    print(f\"Action Space: {env.action_space}\")\n",
                "    return env\n",
                "\n",
                "# Create the environment\n",
                "env = create_environment()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a transition tuple structure\n",
                "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
                "\n",
                "class ReplayMemory:\n",
                "    \"\"\"Experience replay memory for storing and sampling transitions\"\"\"\n",
                "    \n",
                "    def __init__(self, capacity):\n",
                "        \"\"\"Initialize a replay memory with given capacity\"\"\"\n",
                "        self.memory = deque(maxlen=capacity)\n",
                "        \n",
                "    def push(self, *args):\n",
                "        \"\"\"Save a transition to the replay memory\"\"\"\n",
                "        self.memory.append(Transition(*args))\n",
                "        \n",
                "    def sample(self, batch_size):\n",
                "        \"\"\"Randomly sample a batch of transitions from the replay memory\"\"\"\n",
                "        return random.sample(self.memory, batch_size)\n",
                "    \n",
                "    def __len__(self):\n",
                "        \"\"\"Return the current size of the replay memory\"\"\"\n",
                "        return len(self.memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QNetwork(nn.Module):\n",
                "    \"\"\"Neural network for approximating Q-values\"\"\"\n",
                "    \n",
                "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
                "        \"\"\"Initialize the Q-Network\n",
                "        \n",
                "        Args:\n",
                "            input_dim: Dimension of the input (state space)\n",
                "            output_dim: Dimension of the output (action space)\n",
                "            hidden_dim: Size of the hidden layer\n",
                "        \"\"\"\n",
                "        super(QNetwork, self).__init__()\n",
                "        \n",
                "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        \"\"\"Forward pass through the network\"\"\"\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = F.relu(self.fc2(x))\n",
                "        return self.fc3(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DQNAgent:\n",
                "    \"\"\"Deep Q-Network Agent\"\"\"\n",
                "    \n",
                "    def __init__(self, state_size, action_size, hidden_size=128, \n",
                "                 learning_rate=1e-3, gamma=0.99, epsilon_start=1.0, \n",
                "                 epsilon_end=0.01, epsilon_decay=0.995, memory_size=10000, \n",
                "                 batch_size=64, target_update=10, device='cpu'):\n",
                "        \"\"\"Initialize the DQN Agent\n",
                "        \n",
                "        Args:\n",
                "            state_size: Size of the state space\n",
                "            action_size: Size of the action space\n",
                "            hidden_size: Size of the hidden layers in the Q-Network\n",
                "            learning_rate: Learning rate for the optimizer\n",
                "            gamma: Discount factor\n",
                "            epsilon_start: Starting value of epsilon for exploration\n",
                "            epsilon_end: Minimum value of epsilon\n",
                "            epsilon_decay: Rate at which epsilon decays\n",
                "            memory_size: Capacity of the replay memory\n",
                "            batch_size: Size of the batches sampled from the replay memory\n",
                "            target_update: How often to update the target network\n",
                "            device: Device to run the networks on ('cpu' or 'cuda')\n",
                "        \"\"\"\n",
                "        self.state_size = state_size\n",
                "        self.action_size = action_size\n",
                "        self.hidden_size = hidden_size\n",
                "        self.learning_rate = learning_rate\n",
                "        self.gamma = gamma\n",
                "        self.epsilon = epsilon_start\n",
                "        self.epsilon_end = epsilon_end\n",
                "        self.epsilon_decay = epsilon_decay\n",
                "        self.memory_size = memory_size\n",
                "        self.batch_size = batch_size\n",
                "        self.target_update = target_update\n",
                "        self.device = device\n",
                "        \n",
                "        # Initialize Q-Networks (policy and target)\n",
                "        self.policy_net = QNetwork(state_size, action_size, hidden_size).to(device)\n",
                "        self.target_net = QNetwork(state_size, action_size, hidden_size).to(device)\n",
                "        \n",
                "        # Initialize target network with the same weights as the policy network\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        self.target_net.eval()  # Set target network to evaluation mode\n",
                "        \n",
                "        # Initialize optimizer\n",
                "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
                "        \n",
                "        # Initialize replay memory\n",
                "        self.memory = ReplayMemory(memory_size)\n",
                "        \n",
                "        # Initialize step counter (for target network updates)\n",
                "        self.steps_done = 0\n",
                "    \n",
                "    def select_action(self, state, evaluation=False):\n",
                "        \"\"\"Select an action using epsilon-greedy policy\n",
                "        \n",
                "        Args:\n",
                "            state: The current state\n",
                "            evaluation: Whether to use exploration or not\n",
                "        \n",
                "        Returns:\n",
                "            The selected action\n",
                "        \"\"\"\n",
                "        # Convert state to tensor\n",
                "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
                "        \n",
                "        # During evaluation, use greedy policy\n",
                "        if evaluation or random.random() > self.epsilon:\n",
                "            with torch.no_grad():\n",
                "                # t.max(1) returns the maximum value and index for each row\n",
                "                # [1] gets the indices, which are the actions\n",
                "                return self.policy_net(state).max(1)[1].item()\n",
                "        else:\n",
                "            # Random action\n",
                "            return random.randrange(self.action_size)\n",
                "    \n",
                "    def store_transition(self, state, action, next_state, reward, done):\n",
                "        \"\"\"Store a transition in the replay memory\n",
                "        \n",
                "        Args:\n",
                "            state: The current state\n",
                "            action: The action taken\n",
                "            next_state: The next state\n",
                "            reward: The reward received\n",
                "            done: Whether the episode is done\n",
                "        \"\"\"\n",
                "        self.memory.push(state, action, next_state, reward, done)\n",
                "    \n",
                "    def learn(self):\n",
                "        \"\"\"Update the Q-Network using a batch of experiences\"\"\"\n",
                "        # If we don't have enough experiences yet, return\n",
                "        if len(self.memory) < self.batch_size:\n",
                "            return\n",
                "        \n",
                "        # Sample a batch of transitions from the replay memory\n",
                "        transitions = self.memory.sample(self.batch_size)\n",
                "        \n",
                "        # Convert batch of transitions to transition of batches\n",
                "        batch = Transition(*zip(*transitions))\n",
                "        \n",
                "        # Convert to tensors\n",
                "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
                "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
                "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
                "        \n",
                "        # Convert next_states to tensor, handling terminal states\n",
                "        non_final_mask = torch.BoolTensor([not done for done in batch.done]).to(self.device)\n",
                "        non_final_next_states = torch.FloatTensor([s for s, done in zip(batch.next_state, batch.done) \n",
                "                                                 if not done]).to(self.device)\n",
                "        \n",
                "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
                "        # columns of actions taken\n",
                "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
                "        \n",
                "        # Compute V(s_{t+1}) for all next states\n",
                "        next_state_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
                "        \n",
                "        if len(non_final_next_states) > 0:  # Make sure we have non-terminal states\n",
                "            with torch.no_grad():\n",
                "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].unsqueeze(1)\n",
                "        \n",
                "        # Compute the expected Q values\n",
                "        expected_state_action_values = reward_batch + (self.gamma * next_state_values)\n",
                "        \n",
                "        # Compute loss (Huber loss)\n",
                "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
                "        \n",
                "        # Optimize the model\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        # Clip gradients to help with training stability\n",
                "        for param in self.policy_net.parameters():\n",
                "            param.grad.data.clamp_(-1, 1)\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        # Update the target network\n",
                "        self.steps_done += 1\n",
                "        if self.steps_done % self.target_update == 0:\n",
                "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        \n",
                "        # Decay epsilon\n",
                "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
                "        \n",
                "        return loss.item()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop\n",
                "\n",
                "Now, let's implement the training loop to train our DQN agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_dqn(env, agent, num_episodes=1000, max_steps=1000, \n",
                "              print_every=10, render=False, early_stop_reward=None):\n",
                "    \"\"\"Train the DQN agent\n",
                "    \n",
                "    Args:\n",
                "        env: The environment\n",
                "        agent: The DQN agent\n",
                "        num_episodes: Number of episodes to train\n",
                "        max_steps: Maximum steps per episode\n",
                "        print_every: How often to print/plot results\n",
                "        render: Whether to render the environment\n",
                "        early_stop_reward: Reward threshold for early stopping\n",
                "        \n",
                "    Returns:\n",
                "        List of rewards per episode\n",
                "    \"\"\"\n",
                "    # Lists to store results\n",
                "    rewards = []\n",
                "    avg_rewards = []\n",
                "    losses = []\n",
                "    epsilons = []\n",
                "    \n",
                "    for episode in range(num_episodes):\n",
                "        state, _ = env.reset()\n",
                "        episode_reward = 0\n",
                "        episode_loss = 0\n",
                "        step_count = 0\n",
                "        \n",
                "        for step in range(max_steps):\n",
                "            # Select action\n",
                "            action = agent.select_action(state)\n",
                "            \n",
                "            # Take action and observe next state and reward\n",
                "            next_state, reward, done, truncated, _ = env.step(action)\n",
                "            \n",
                "            # Store transition in replay memory\n",
                "            agent.store_transition(state, action, next_state, reward, done)\n",
                "            \n",
                "            # Learn from experience\n",
                "            loss = agent.learn()\n",
                "            if loss is not None:\n",
                "                episode_loss += loss\n",
                "            \n",
                "            # Update state and reward\n",
                "            state = next_state\n",
                "            episode_reward += reward\n",
                "            step_count += 1\n",
                "            \n",
                "            # Render if specified\n",
                "            if render and episode % print_every == 0:\n",
                "                env.render()\n",
                "                time.sleep(0.01)\n",
                "            \n",
                "            # Break if episode is done\n",
                "            if done or truncated:\n",
                "                break\n",
                "        \n",
                "        # Append results\n",
                "        rewards.append(episode_reward)\n",
                "        losses.append(episode_loss / step_count if step_count > 0 else 0)\n",
                "        epsilons.append(agent.epsilon)\n",
                "        \n",
                "        # Calculate moving average of rewards\n",
                "        window = min(len(rewards), 100)\n",
                "        avg_reward = sum(rewards[-window:]) / window\n",
                "        avg_rewards.append(avg_reward)\n",
                "        \n",
                "        # Print and plot results\n",
                "        if (episode + 1) % print_every == 0 or episode == 0:\n",
                "            print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
                "                  f\"Reward: {episode_reward:.2f}, \"\n",
                "                  f\"Avg Reward: {avg_reward:.2f}, \"\n",
                "                  f\"Epsilon: {agent.epsilon:.4f}, \"\n",
                "                  f\"Steps: {step_count}\")\n",
                "            \n",
                "            # Plot training progress\n",
                "            clear_output(wait=True)\n",
                "            plt.figure(figsize=(15, 10))\n",
                "            \n",
                "            # Plot rewards\n",
                "            plt.subplot(2, 2, 1)\n",
                "            plt.plot(rewards, label='Episode Reward')\n",
                "            plt.plot(avg_rewards, label='Avg Reward (100 episodes)')\n",
                "            if early_stop_reward is not None:\n",
                "                plt.axhline(y=early_stop_reward, color='r', linestyle='--', label='Target Reward')\n",
                "            plt.xlabel('Episode')\n",
                "            plt.ylabel('Reward')\n",
                "            plt.title('Rewards')\n",
                "            plt.legend()\n",
                "            \n",
                "            # Plot losses\n",
                "            plt.subplot(2, 2, 2)\n",
                "            plt.plot(losses)\n",
                "            plt.xlabel('Episode')\n",
                "            plt.ylabel('Loss')\n",
                "            plt.title('Training Loss')\n",
                "            \n",
                "            # Plot epsilon\n",
                "            plt.subplot(2, 2, 3)\n",
                "            plt.plot(epsilons)\n",
                "            plt.xlabel('Episode')\n",
                "            plt.ylabel('Epsilon')\n",
                "            plt.title('Exploration Rate (Epsilon)')\n",
                "            \n",
                "            # Plot histogram of rewards\n",
                "            plt.subplot(2, 2, 4)\n",
                "            plt.hist(rewards, bins=20)\n",
                "            plt.xlabel('Reward')\n",
                "            plt.ylabel('Frequency')\n",
                "            plt.title('Reward Distribution')\n",
                "            \n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "        \n",
                "        # Check for early stopping\n",
                "        if early_stop_reward is not None and avg_reward >= early_stop_reward:\n",
                "            print(f\"\\nEnvironment solved in {episode + 1} episodes!\")\n",
                "            print(f\"Average reward over the last 100 episodes: {avg_reward:.2f}\")\n",
                "            break\n",
                "    \n",
                "    return rewards, avg_rewards, losses, epsilons"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation Function\n",
                "\n",
                "Let's implement a function to evaluate the trained agent's performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_agent(env, agent, num_episodes=10, render=True):\n",
                "    \"\"\"Evaluate the trained agent\n",
                "    \n",
                "    Args:\n",
                "        env: The environment\n",
                "        agent: The DQN agent\n",
                "        num_episodes: Number of episodes to evaluate\n",
                "        render: Whether to render the environment\n",
                "        \n",
                "    Returns:\n",
                "        Average reward over the evaluation episodes\n",
                "    \"\"\"\n",
                "    total_rewards = []\n",
                "    \n",
                "    for episode in range(num_episodes):\n",
                "        state, _ = env.reset()\n",
                "        episode_reward = 0\n",
                "        done = False\n",
                "        truncated = False\n",
                "        \n",
                "        # Create a render environment if rendering is enabled\n",
                "        if render:\n",
                "            eval_env = gym.make(env.spec.id, render_mode='human')\n",
                "            eval_state, _ = eval_env.reset()\n",
                "        else:\n",
                "            eval_env = env\n",
                "            eval_state = state\n",
                "        \n",
                "        while not (done or truncated):\n",
                "            # Select action (greedy policy for evaluation)\n",
                "            action = agent.select_action(eval_state, evaluation=True)\n",
                "            \n",
                "            # Take action\n",
                "            eval_state, reward, done, truncated, _ = eval_env.step(action)\n",
                "            episode_reward += reward\n",
                "        \n",
                "        if render:\n",
                "            eval_env.close()\n",
                "        \n",
                "        total_rewards.append(episode_reward)\n",
                "        print(f\"Evaluation Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
                "    \n",
                "    avg_reward = sum(total_rewards) / num_episodes\n",
                "    print(f\"\\nAverage Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
                "    \n",
                "    return avg_reward"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training the Agent\n",
                "\n",
                "Now, let's train the DQN agent on the CartPole environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the environment\n",
                "env = create_environment(\"CartPole-v1\")\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Create the DQN agent\n",
                "state_size = env.observation_space.shape[0]  # Assuming continuous state space\n",
                "action_size = env.action_space.n  # Assuming discrete action space\n",
                "\n",
                "agent = DQNAgent(\n",
                "    state_size=state_size,\n",
                "    action_size=action_size,\n",
                "    hidden_size=128,\n",
                "    learning_rate=1e-3,\n",
                "    gamma=0.99,\n",
                "    epsilon_start=1.0,\n",
                "    epsilon_end=0.01,\n",
                "    epsilon_decay=0.995,\n",
                "    memory_size=10000,\n",
                "    batch_size=64,\n",
                "    target_update=10,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Train the agent\n",
                "rewards, avg_rewards, losses, epsilons = train_dqn(\n",
                "    env=env,\n",
                "    agent=agent,\n",
                "    num_episodes=500,\n",
                "    print_every=10,\n",
                "    early_stop_reward=195.0  # CartPole-v1 is considered solved when avg reward is 195+ over 100 episodes\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluating the Trained Agent\n",
                "\n",
                "Let's evaluate the performance of our trained agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the trained agent\n",
                "avg_reward = evaluate_agent(env, agent, num_episodes=5, render=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Extensions and Experiments\n",
                "\n",
                "Here are some suggested extensions and experiments to try:\n",
                "\n",
                "1. Implement Double DQN\n",
                "2. Implement Dueling DQN\n",
                "3. Experiment with different network architectures\n",
                "4. Try different environments (e.g., LunarLander-v2)\n",
                "5. Implement prioritized experience replay\n",
                "6. Compare DQN with other algorithms (e.g., REINFORCE, A2C)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Implement Double DQN\n",
                "class DoubleDQNAgent(DQNAgent):\n",
                "    \"\"\"Double DQN Agent that uses two networks to reduce overestimation\"\"\"\n",
                "    \n",
                "    def learn(self):\n",
                "        \"\"\"Update the Q-Network using Double DQN\"\"\"\n",
                "        # TODO: Implement Double DQN learning\n",
                "        pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Implement Dueling DQN\n",
                "class DuelingQNetwork(nn.Module):\n",
                "    \"\"\"Dueling Q-Network architecture\"\"\"\n",
                "    \n",
                "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
                "        \"\"\"Initialize the Dueling Q-Network\"\"\"\n",
                "        super(DuelingQNetwork, self).__init__()\n",
                "        \n",
                "        # TODO: Implement Dueling DQN architecture\n",
                "        pass\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Forward pass through the network\"\"\"\n",
                "        # TODO: Implement forward pass\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Conclusion\n",
                "\n",
                "In this notebook, we implemented and trained a Deep Q-Network (DQN) agent to solve the CartPole environment. We explored the key components of DQN, including the Q-Network architecture, experience replay, and target networks.\n",
                "\n",
                "What we learned:\n",
                "- How to implement a DQN agent using PyTorch\n",
                "- How to use experience replay to improve sample efficiency\n",
                "- How to stabilize training using target networks\n",
                "- How to evaluate the performance of a RL agent\n",
                "\n",
                "Future directions:\n",
                "- Implement more advanced algorithms (Double DQN, Dueling DQN, etc.)\n",
                "- Try more complex environments\n",
                "- Experiment with different hyperparameters and network architectures"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
