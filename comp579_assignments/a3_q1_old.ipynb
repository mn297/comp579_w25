{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (76.0.0)\n",
      "Requirement already satisfied: ale-py in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from ale-py) (1.26.4)\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (2.5.0)\n",
      "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3)\n",
      "  Using cached gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from stable-baselines3) (2.4.1+cu118)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from stable-baselines3) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.15.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (76.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from sympy->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Using cached gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.1.1\n",
      "    Uninstalling gymnasium-1.1.1:\n",
      "      Successfully uninstalled gymnasium-1.1.1\n",
      "Successfully installed gymnasium-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale-py>=0.9 in c:\\users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools ale-py\n",
    "%pip install stable-baselines3\n",
    "%pip install \"gymnasium[accept-rom-license, atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "\n",
    "# import gym\n",
    "# from gym.wrappers import StepAPICompatibility\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True  # Set to False to force CPU usage\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    if use_gpu:\n",
    "        print(\"GPU requested but not available. Using CPU instead.\")\n",
    "    else:\n",
    "        print(\"Using CPU as requested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "State space: Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n"
     ]
    }
   ],
   "source": [
    "acrobot_env_name = 'Acrobot-v1'\n",
    "acrobot_env = gym.make(acrobot_env_name)\n",
    "# acrobot_env = StepAPICompatibility(acrobot_env)\n",
    "print(\"Action space:\", acrobot_env.action_space)\n",
    "print(\"State space:\", acrobot_env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(7)\n",
      "State space: Box(0, 255, (128,), uint8)\n"
     ]
    }
   ],
   "source": [
    "assault_env_name = \"ALE/Assault-ram-v5\"\n",
    "assault_env = gym.make(assault_env_name)\n",
    "# assault_env = StepAPICompatibility(assault_env)\n",
    "print(\"Action space:\", assault_env.action_space)\n",
    "print(\"State space:\", assault_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temp):\n",
    "    z = np.exp(x / temp - np.max(x / temp))\n",
    "    return z / np.sum(z)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "        self.mlp.apply(self.init_weights)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.uniform_(m.weight, -0.001, 0.001)\n",
    "            nn.init.uniform_(m.bias, -0.001, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class DeepValueLearning:\n",
    "    def __init__(self, env, step_size, epsilon, algorithm, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.algorithm = algorithm\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = \"cpu\"\n",
    "        self.Q = QNetwork(self.state_dim, self.n_actions, self.device)\n",
    "        self.optimizer = optim.SGD(self.Q.parameters(), lr=self.step_size)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, s):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            state_input = torch.tensor(s).float().unsqueeze(0).to(self.device)\n",
    "            return torch.argmax(self.Q(state_input)).cpu().item()\n",
    "\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "    ):\n",
    "        state_batch = torch.tensor(state_batch).float().to(self.device)\n",
    "        action_batch = torch.tensor(action_batch).long().to(self.device)\n",
    "        reward_batch = torch.tensor(reward_batch).float().to(self.device)\n",
    "        next_state_batch = torch.tensor(next_state_batch).float().to(self.device)\n",
    "        done_batch = torch.tensor(done_batch).bool().to(self.device)\n",
    "\n",
    "        q_val_batch = (\n",
    "            self.Q(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_val = self.Q(next_state_batch)\n",
    "            greedy_next_q_val = next_q_val.max(1)[0]\n",
    "            if self.algorithm == \"Q-Learning\":\n",
    "                target_batch = torch.where(\n",
    "                    done_batch,\n",
    "                    reward_batch,\n",
    "                    reward_batch + self.gamma * greedy_next_q_val,\n",
    "                )\n",
    "            else:\n",
    "                random_next_q_val = next_q_val.mean(dim=1)\n",
    "                exp_next_q_val = (\n",
    "                    self.epsilon * random_next_q_val\n",
    "                    + (1 - self.epsilon) * greedy_next_q_val\n",
    "                )\n",
    "                target_batch = torch.where(\n",
    "                    done_batch, reward_batch, reward_batch + self.gamma * exp_next_q_val\n",
    "                )\n",
    "\n",
    "        loss = self.loss_fn(q_val_batch, target_batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial # 1 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [25:34<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/720 trials\n",
      "Starting trial # 2 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [21:44<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2/720 trials\n",
      "Starting trial # 3 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [27:12<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 3/720 trials\n",
      "Starting trial # 4 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [26:35<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 4/720 trials\n",
      "Starting trial # 5 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [25:46<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5/720 trials\n",
      "Starting trial # 6 / 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 102/1000 [02:30<22:08,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 89\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting trial #\u001b[39m\u001b[38;5;124m\"\u001b[39m, trials_completed \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_trials\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 89\u001b[0m     episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_buffer\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     results[\n\u001b[0;32m     93\u001b[0m         (env_name, seed, epsilon, step_size, algorithm, use_buffer)\n\u001b[0;32m     94\u001b[0m     ] \u001b[38;5;241m=\u001b[39m episode_rewards\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mrun_trial\u001b[1;34m(epsilon, step_size, seed, env, algorithm, use_buffer)\u001b[0m\n\u001b[0;32m     29\u001b[0m         transitions \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(replay_minibatch_size)\n\u001b[0;32m     30\u001b[0m         (\n\u001b[0;32m     31\u001b[0m             state_batch,\n\u001b[0;32m     32\u001b[0m             action_batch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m             done_batch,\n\u001b[0;32m     36\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions)\n\u001b[1;32m---> 37\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdone_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate([state], [action], [reward], [next_state], [done])\n",
      "Cell \u001b[1;32mIn[12], line 56\u001b[0m, in \u001b[0;36mDeepValueLearning.update\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n\u001b[0;32m     55\u001b[0m ):\n\u001b[1;32m---> 56\u001b[0m     state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action_batch)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     58\u001b[0m     reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward_batch)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_trial(epsilon, step_size, seed, env, algorithm, use_buffer):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    agent = DeepValueLearning(env, step_size, epsilon, algorithm)\n",
    "\n",
    "    if use_buffer:\n",
    "        replay_buffer = ReplayBuffer(1_000_000)\n",
    "        replay_minibatch_size = 128\n",
    "\n",
    "    max_steps_per_episode = 500\n",
    "\n",
    "    episode_rewards = []\n",
    "    for _ in tqdm(range(1000)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        n_steps = 0\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            if use_buffer:\n",
    "                replay_buffer.push((state, action, reward, next_state, done))\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if use_buffer:\n",
    "                if (\n",
    "                    n_steps % replay_minibatch_size\n",
    "                    and len(replay_buffer) > replay_minibatch_size\n",
    "                ):\n",
    "                    # if len(replay_buffer) > replay_minibatch_size:\n",
    "                    transitions = replay_buffer.sample(replay_minibatch_size)\n",
    "                    (\n",
    "                        state_batch,\n",
    "                        action_batch,\n",
    "                        reward_batch,\n",
    "                        next_state_batch,\n",
    "                        done_batch,\n",
    "                    ) = zip(*transitions)\n",
    "                    agent.update(\n",
    "                        state_batch,\n",
    "                        action_batch,\n",
    "                        reward_batch,\n",
    "                        next_state_batch,\n",
    "                        done_batch,\n",
    "                    )\n",
    "            else:\n",
    "                agent.update([state], [action], [reward], [next_state], [done])\n",
    "\n",
    "            state = next_state\n",
    "            n_steps += 1\n",
    "            if n_steps >= max_steps_per_episode:\n",
    "                done = True\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "epsilons = [0.01, 0.1, 0.5]\n",
    "step_sizes = [1 / 4, 1 / 8, 1 / 16]\n",
    "seeds = range(10)\n",
    "envs = [acrobot_env, assault_env]\n",
    "algorithms = [\"Expected-SARSA\", \"Q-Learning\"]\n",
    "\n",
    "total_trials = (\n",
    "    len(envs) * len(seeds) * len(epsilons) * len(step_sizes) * len(algorithms) * 2\n",
    ")\n",
    "\n",
    "\n",
    "# load pickle file for results\n",
    "# check if file exists\n",
    "try:\n",
    "    with open(\"results.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "        trials_completed = len(results)\n",
    "except:\n",
    "    results = {}\n",
    "    trials_completed = 0\n",
    "\n",
    "for env in envs:\n",
    "    env_name = env.env.spec.id\n",
    "    for epsilon in epsilons:\n",
    "        for step_size in step_sizes:\n",
    "            for algorithm in algorithms:\n",
    "                for use_buffer in [True, False]:\n",
    "                    for seed in seeds:\n",
    "                        print(\n",
    "                            \"Starting trial #\", trials_completed + 1, \"/\", total_trials\n",
    "                        )\n",
    "\n",
    "                        episode_rewards = run_trial(\n",
    "                            epsilon, step_size, seed, env, algorithm, use_buffer\n",
    "                        )\n",
    "                        results[\n",
    "                            (env_name, seed, epsilon, step_size, algorithm, use_buffer)\n",
    "                        ] = episode_rewards\n",
    "                        with open(\"results.pkl\", \"wb\") as f:\n",
    "                            pickle.dump(results, f)\n",
    "                        trials_completed += 1\n",
    "                        print(f\"Completed {trials_completed}/{total_trials} trials\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
