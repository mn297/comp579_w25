{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# PLEASE NOTE:\n",
    "# 1) This is a simple demonstration of a daily backtest using a \"dumb\" MACD+RSI+Oscillator signal.\n",
    "# 2) The code downloads historical data for a given ticker (default: AAPL) from Yahoo Finance.\n",
    "# 3) No advanced error handling or transaction cost modeling is included here.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "import yfinance as yf  # For downloading Yahoo Finance data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------\n",
    "#  DOWNLOAD HISTORICAL DATA FROM YAHOO FINANCE\n",
    "# --------------------\n",
    "ticker = \"AAPL\"       # Replace with your desired ticker\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-01-01\"\n",
    "df = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Ensure the DataFrame has the expected columns: 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'\n",
    "# We'll work with 'Close', 'High', and 'Low' for our indicators.\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "#  HELPER FUNCTIONS: MACD, RSI, STOCHASTIC OSCILLATOR\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def compute_MACD(data, short_window=12, long_window=26, signal_window=9):\n",
    "    \"\"\"\n",
    "    Compute MACD (Moving Average Convergence Divergence).\n",
    "    MACD = EMA(short_window) - EMA(long_window)\n",
    "    signal = EMA(MACD, signal_window)\n",
    "    \"\"\"\n",
    "    short_ema = data['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = data['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()\n",
    "    macd_hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, macd_hist\n",
    "\n",
    "def compute_RSI(data, period=14):\n",
    "    \"\"\"\n",
    "    Compute the Relative Strength Index (RSI).\n",
    "    RSI < 30 => oversold\n",
    "    RSI > 70 => overbought\n",
    "    \"\"\"\n",
    "    delta = data['Close'].diff()\n",
    "    gain = np.where(delta > 0, delta, 0.0)\n",
    "    loss = np.where(delta < 0, -delta, 0.0)\n",
    "    avg_gain = pd.Series(gain).rolling(window=period).mean()\n",
    "    avg_loss = pd.Series(loss).rolling(window=period).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-10)  # Prevent division by zero\n",
    "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
    "    return rsi\n",
    "\n",
    "def compute_Stoch_Osc(data, k_period=14, d_period=3):\n",
    "    \"\"\"\n",
    "    Compute the Stochastic Oscillator.\n",
    "    %K = (Close - LowestLow) / (HighestHigh - LowestLow) * 100\n",
    "    %D = SMA of %K over d_period\n",
    "    \"\"\"\n",
    "    low_min = data['Low'].rolling(k_period).min()\n",
    "    high_max = data['High'].rolling(k_period).max()\n",
    "    stoch_k = 100 * (data['Close'] - low_min) / (high_max - low_min + 1e-10)\n",
    "    stoch_d = stoch_k.rolling(d_period).mean()\n",
    "    return stoch_k, stoch_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (253, 1) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#  CALCULATE INDICATORS\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_Line\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_Signal\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_Hist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m compute_MACD(df)\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRSI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_RSI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStoch_K\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStoch_D\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m compute_Stoch_Osc(df)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#  STRATEGY LOGIC (DUMB EXAMPLE):\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#  1) BUY condition:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize columns for signals and positions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mcompute_RSI\u001b[1;34m(data, period)\u001b[0m\n\u001b[0;32m     47\u001b[0m gain \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(delta \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, delta, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(delta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39mdelta, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m avg_gain \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39mperiod)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     50\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(loss)\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39mperiod)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     51\u001b[0m rs \u001b[38;5;241m=\u001b[39m avg_gain \u001b[38;5;241m/\u001b[39m (avg_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)  \u001b[38;5;66;03m# Prevent division by zero\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\pandas\\core\\series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\pandas\\core\\construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    656\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[0;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32mc:\\Users\\john\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\pandas\\core\\construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m     )\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (253, 1) instead"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------\n",
    "#  CALCULATE INDICATORS\n",
    "# ----------------------------------\n",
    "df['MACD_Line'], df['MACD_Signal'], df['MACD_Hist'] = compute_MACD(df)\n",
    "df['RSI'] = compute_RSI(df)\n",
    "df['Stoch_K'], df['Stoch_D'] = compute_Stoch_Osc(df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "#  STRATEGY LOGIC (DUMB EXAMPLE):\n",
    "#  1) BUY condition:\n",
    "#      - MACD_Line > MACD_Signal (simple bullish MACD crossover)\n",
    "#      - RSI < 70 (avoid overbought conditions)\n",
    "#      - Stoch_K < 80 (avoid overbought conditions, dumb threshold)\n",
    "#  2) SELL condition:\n",
    "#      - Otherwise, be flat (no position).\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize columns for signals and positions\n",
    "df['Signal'] = 0\n",
    "df['Position'] = 0\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    # \"Dumb\" buy rule for day i\n",
    "    if (df['MACD_Line'].iloc[i] > df['MACD_Signal'].iloc[i]) \\\n",
    "       and (df['RSI'].iloc[i] < 70) \\\n",
    "       and (df['Stoch_K'].iloc[i] < 80):\n",
    "        # Buy signal = 1\n",
    "        df.at[df.index[i], 'Signal'] = 1\n",
    "    else:\n",
    "        # Sell/Flat signal = 0\n",
    "        df.at[df.index[i], 'Signal'] = 0\n",
    "\n",
    "# For simplicity, we assume the strategy is either fully invested (1) or flat (0).\n",
    "# We use the previous day's signal to set today's position.\n",
    "df['Position'] = df['Signal'].shift(1).fillna(0)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "#  CALCULATE RETURNS\n",
    "# ---------------------------------------------------------------------------------\n",
    "df['Market_Return'] = df['Close'].pct_change()\n",
    "df['Strategy_Return'] = df['Position'] * df['Market_Return']\n",
    "\n",
    "# Compute cumulative returns\n",
    "df['Cumulative_Market'] = (1 + df['Market_Return']).cumprod() - 1\n",
    "df['Cumulative_Strategy'] = (1 + df['Strategy_Return']).cumprod() - 1\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "#  OUTPUT PERFORMANCE\n",
    "# ---------------------------------------------------------------------------------\n",
    "final_market_return = df['Cumulative_Market'].iloc[-1] * 100\n",
    "final_strategy_return = df['Cumulative_Strategy'].iloc[-1] * 100\n",
    "\n",
    "print(f\"Final Market Return     : {final_market_return:.2f}%\")\n",
    "print(f\"Final Strategy Return   : {final_strategy_return:.2f}%\")\n",
    "\n",
    "# Optional: Inspect final few rows of the DataFrame\n",
    "print(df.tail(10))\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# END OF DEMO\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Required Libraries ---\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta # Technical Analysis library: https://github.com/bukosabino/ta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "# print(f\"TA library version: {ta.__version__}\") # Removed as requested\n",
    "print(f\"yfinance version: {yf.__version__}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TICKER = 'AAPL'         # Example Stock ticker\n",
    "START_DATE = '2018-01-01'\n",
    "# Use current date or a recent date for END_DATE if you want up-to-date data for prediction step\n",
    "# For training/testing on historical data, keep a fixed end date\n",
    "END_DATE = '2024-12-31'\n",
    "PREDICTION_HORIZON = 1  # Predict 1 day ahead (simple binary up/down)\n",
    "TEST_SIZE = 0.2         # 20% of data for testing\n",
    "RANDOM_STATE = 42       # For reproducible train/test split\n",
    "\n",
    "# NN Hyperparameters\n",
    "INPUT_SIZE = 0          # Will be set based on number of features\n",
    "HIDDEN_SIZE = 32        # Arbitrary number of neurons in hidden layer\n",
    "OUTPUT_SIZE = 1         # Binary classification (Up=1, Down=0)\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "\n",
    "# --- 1. Data Acquisition ---\n",
    "print(f\"\\n[1] Downloading data for {TICKER}...\")\n",
    "try:\n",
    "    # Use auto_adjust=True to get adjusted prices ('close' will be adj close)\n",
    "    df = yf.download(TICKER, start=START_DATE, end=END_DATE, auto_adjust=True)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No data downloaded. Check ticker and date range.\")\n",
    "    print(f\"Downloaded {len(df)} data points.\")\n",
    "    # yfinance with auto_adjust=True already gives lowercase column names\n",
    "    # df.columns = [str(col).lower().replace(' ', '_') for col in df.columns] # Less needed now\n",
    "    print(\"Original Columns:\", df.columns)\n",
    "    print(df.tail())\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Engineering (Add Technical Indicators) ---\n",
    "print(\"\\n[2] Calculating Technical Indicators...\")\n",
    "\n",
    "# Add TA features using the 'ta' library\n",
    "# Ensure the necessary columns ('open', 'high', 'low', 'close', 'volume') exist\n",
    "required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    print(f\"Error: DataFrame missing required columns for TA. Found: {df.columns}. Need: {required_cols}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = ta.add_all_ta_features(\n",
    "        df, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during ta.add_all_ta_features: {e}\")\n",
    "    print(\"DataFrame columns before error:\", df.columns)\n",
    "    exit()\n",
    "\n",
    "# --- FIX for MultiIndex Columns and Column Cleaning ---\n",
    "# Check if 'ta' library created a MultiIndex and flatten if necessary\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    print(\"Detected MultiIndex columns after TA, flattening...\")\n",
    "    # Join levels with an underscore, ensuring all parts are strings\n",
    "    df.columns = ['_'.join(map(str, col)).strip().strip('_') for col in df.columns.values] # Added strip('_')\n",
    "\n",
    "# Clean column names (lowercase, replace invalid chars) - applied AFTER potential flattening\n",
    "# Make sure this cleaning process doesn't mangle essential names like 'close'\n",
    "df.columns = [str(col).lower().replace(' ', '_').replace('-', '_').replace(':', '_').replace('%', 'perc') for col in df.columns]\n",
    "# --- End FIX ---\n",
    "\n",
    "print(\"Columns after TA and cleaning:\", df.columns) # Show columns after processing\n",
    "\n",
    "# --- *** ADDED CHECK/FIX FOR 'close' COLUMN *** ---\n",
    "# Ensure 'close' column exists after all manipulations\n",
    "if 'close' not in df.columns:\n",
    "    print(\"Warning: 'close' column not found directly after cleaning. Checking for variations...\")\n",
    "    # Common variations if flattening occurred (e.g., from ('Close', ''))\n",
    "    potential_close_names = ['close_', '_close']\n",
    "    found_close = False\n",
    "    for name in potential_close_names:\n",
    "        if name in df.columns:\n",
    "            print(f\"Found '{name}', renaming to 'close'.\")\n",
    "            df.rename(columns={name: 'close'}, inplace=True)\n",
    "            found_close = True\n",
    "            break\n",
    "    if not found_close:\n",
    "        print(\"\\nFATAL Error: Cannot find 'close' column or known variations after processing.\")\n",
    "        print(\"This is needed for creating the target variable.\")\n",
    "        print(\"Available columns:\", sorted(list(df.columns)))\n",
    "        exit()\n",
    "# --- *** END CHECK/FIX *** ---\n",
    "\n",
    "\n",
    "# Manually select a smaller subset of features for simplicity\n",
    "feature_columns = [\n",
    "    'momentum_rsi',        # Relative Strength Index\n",
    "    'trend_macd_diff',     # MACD Difference\n",
    "    'volatility_bbhi',     # Bollinger Band High Indicator (1 if close > high band)\n",
    "    'volatility_bbli',     # Bollinger Band Low Indicator (1 if close < low band)\n",
    "    'momentum_stoch',      # Stochastic Oscillator %K\n",
    "]\n",
    "\n",
    "# Ensure selected feature columns exist after ta.add_all_ta_features() and cleaning\n",
    "available_cols = [col for col in feature_columns if col in df.columns]\n",
    "if len(available_cols) != len(feature_columns):\n",
    "    print(\"\\nWarning: Some selected feature columns are not available in the DataFrame.\")\n",
    "    missing = set(feature_columns) - set(available_cols)\n",
    "    print(f\"Missing or differently named features: {missing}\")\n",
    "    print(\"Re-check feature_columns list against 'Columns after TA and cleaning' printed above.\")\n",
    "\n",
    "feature_columns = available_cols # Use only available cols\n",
    "INPUT_SIZE = len(feature_columns) # Update input size for NN\n",
    "\n",
    "if INPUT_SIZE == 0:\n",
    "    print(\"\\nError: No valid feature columns selected or generated. Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nSelected {INPUT_SIZE} features for model: {feature_columns}\")\n",
    "\n",
    "\n",
    "# --- 3. Data Preparation ---\n",
    "print(\"\\n[3] Preparing data for Neural Network...\")\n",
    "\n",
    "# 3.1 Create Target Variable: Predict if next day's close is higher than today's close\n",
    "# This line should now work as we ensured 'close' column exists\n",
    "try:\n",
    "    df['target'] = (df['close'].shift(-PREDICTION_HORIZON) > df['close']).astype(int)\n",
    "except KeyError as e:\n",
    "     print(f\"\\nFATAL KeyError: Still cannot find 'close' column right before creating target, even after checks.\")\n",
    "     print(\"Columns at point of error:\", df.columns)\n",
    "     # It might be useful to see the last few rows where 'close' might be missing\n",
    "     print(\"\\nTail of DataFrame before error:\")\n",
    "     print(df.tail())\n",
    "     raise e # Re-raise the error after printing context\n",
    "\n",
    "# 3.2 Drop rows with NaN values\n",
    "original_len = len(df)\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Dropped {original_len - len(df)} rows with NaN values (from TA calculations and target shift).\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"\\nError: DataFrame is empty after dropping NaN values. Need more data or different indicators.\")\n",
    "    exit()\n",
    "\n",
    "# 3.3 Select Features (X) and Target (y)\n",
    "X = df[feature_columns]\n",
    "y = df['target']\n",
    "\n",
    "# 3.4 Split Data into Training and Testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=False # DO NOT shuffle time series data\n",
    ")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# 3.5 Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3.6 Convert data to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# --- 4. Define the Simple Neural Network ---\n",
    "print(\"\\n[4] Defining the Neural Network...\")\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "print(model)\n",
    "\n",
    "# --- 5. Training the Model ---\n",
    "print(\"\\n[5] Training the Model...\")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 6. Evaluate the Model (Basic Accuracy) ---\n",
    "print(\"\\n[6] Evaluating the Model on Test Data...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    predicted = (test_outputs > 0.5).float()\n",
    "    accuracy = (predicted == y_test_tensor).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy.item():.4f}')\n",
    "    naive_accuracy = max(y_test.mean(), 1 - y_test.mean()) if not y_test.empty else 0.5\n",
    "    print(f'Naive Benchmark Accuracy (predict majority class): {naive_accuracy:.4f}')\n",
    "\n",
    "# --- 7. \"Dumb\" Prediction for the Next Day (Illustrative) ---\n",
    "print(\"\\n[7] Making a 'Dumb' Prediction for the Day After Last Data Point...\")\n",
    "# Re-fetch or use the full dataframe state *before* dropna was called to get the last features\n",
    "try:\n",
    "    # Option 1: Re-create the full feature set (safer if df was modified extensively)\n",
    "    df_full = yf.download(TICKER, start=START_DATE, end=END_DATE, auto_adjust=True)\n",
    "    # df_full.columns = [str(col).lower().replace(' ', '_') for col in df_full.columns] # Less needed\n",
    "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in df_full.columns for col in required_cols):\n",
    "         raise ValueError(\"Required columns missing in re-downloaded data for prediction.\")\n",
    "    df_full = ta.add_all_ta_features(df_full, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "    if isinstance(df_full.columns, pd.MultiIndex):\n",
    "        df_full.columns = ['_'.join(map(str, col)).strip().strip('_') for col in df_full.columns.values]\n",
    "    df_full.columns = [str(col).lower().replace(' ', '_').replace('-', '_').replace(':', '_').replace('%', 'perc') for col in df_full.columns]\n",
    "\n",
    "    # Ensure the required feature_columns exist in this df_full\n",
    "    if not all(col in df_full.columns for col in feature_columns):\n",
    "        missing_pred_features = set(feature_columns) - set(df_full.columns)\n",
    "        raise ValueError(f\"Features needed for prediction missing after re-processing: {missing_pred_features}\")\n",
    "\n",
    "    # Select the last row that has valid values for the chosen features\n",
    "    last_valid_index = df_full[feature_columns].last_valid_index()\n",
    "\n",
    "    if last_valid_index is not None:\n",
    "        last_data_point = df_full.loc[[last_valid_index]][feature_columns]\n",
    "\n",
    "        # Scale the last data point using the *same* scaler fitted on training data\n",
    "        last_data_scaled = scaler.transform(last_data_point) # scaler expects a 2D array\n",
    "        last_data_tensor = torch.tensor(last_data_scaled, dtype=torch.float32)\n",
    "\n",
    "        # Make prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction_prob = model(last_data_tensor)\n",
    "            prediction_class = (prediction_prob > 0.5).int().item()\n",
    "\n",
    "        print(f\"Features for last available data point ({last_valid_index.date()}):\")\n",
    "        print(last_data_point)\n",
    "        print(f\"\\nModel Output Probability: {prediction_prob.item():.4f}\")\n",
    "        print(f\"Prediction for next day: {'UP' if prediction_class == 1 else 'DOWN'} ({prediction_class})\")\n",
    "    else:\n",
    "         print(\"Could not find a valid last data point (index) with all required features in the re-processed data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during prediction phase: {e}\")\n",
    "    print(\"Could not make prediction for the next day.\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished. Remember: This is a highly simplified example and NOT financial advice.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
