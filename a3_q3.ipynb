{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import StepAPICompatibility\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acrobot_env_name = 'Acrobot-v1'\n",
    "acrobot_env = gym.make(acrobot_env_name)\n",
    "# acrobot_env = StepAPICompatibility(acrobot_env)\n",
    "print(\"Action space:\", acrobot_env.action_space)\n",
    "print(\"State space:\", acrobot_env.observation_space)\n",
    "print(acrobot_env.env.spec.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assault_env_name = \"ALE/Assault-ram-v5\"\n",
    "assault_env = gym.make(assault_env_name)\n",
    "# assault_env = StepAPICompatibility(assault_env)\n",
    "print(\"Action space:\", assault_env.action_space)\n",
    "print(\"State space:\", assault_env.observation_space)\n",
    "print(assault_env.env.spec.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, device=\"cpu\"):\n",
    "        super(ZNetwork, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.FloatTensor(x)\n",
    "            x = x.to(self.device)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# approximates the state-value function using NN\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128, device=\"cpu\"):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.FloatTensor(x)\n",
    "            x = x.to(self.device)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class BoltzmannPolicy:\n",
    "    def __init__(self, z_network, temperature=1.0, device=\"cpu\"):\n",
    "        self.z_network = z_network\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "\n",
    "    # Boltzmann distribution\n",
    "    def select_action(self, state):\n",
    "        # forward ZNetwork\n",
    "        state_input = torch.as_tensor(state).float().unsqueeze(0).to(self.device)\n",
    "        z_s = self.z_network(state_input)\n",
    "        logits = z_s / self.temperature\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # categorical distribution and sample action, exmaple Categorical(probs: tensor([0.1, 0.2, 0.7]))\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # return action.item(), m.log_prob(action), probs.detach().numpy()\n",
    "        return action.item(), m.log_prob(action), probs.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# REINFORCE is the agent that will train the NN\n",
    "class REINFORCE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        lr=0.001,\n",
    "        gamma=0.99,\n",
    "        temperature=1.0,\n",
    "        temp_decay=0.9999,\n",
    "        decreasing_temp=False,\n",
    "        max_steps_per_episode=500,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        if not torch.cuda.is_available():\n",
    "            self.device = \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.z_network = ZNetwork(self.state_dim, self.action_dim, device=self.device)\n",
    "        # Use Adam instead of SGD\n",
    "        self.optimizer = optim.Adam(self.z_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.env_name = self.env.env.spec.id\n",
    "\n",
    "        if temperature == \"decreasing\":\n",
    "            self.initial_temp = 1\n",
    "            self.decreasing_temp = True\n",
    "            self.temp_decay = temp_decay\n",
    "        else:\n",
    "            self.initial_temp = temperature\n",
    "            self.decreasing_temp = decreasing_temp\n",
    "            self.temp_decay = temp_decay\n",
    "\n",
    "        self.policy = BoltzmannPolicy(\n",
    "            self.z_network, self.initial_temp, device=self.device\n",
    "        )\n",
    "\n",
    "    def train_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        if self.env_name == \"ALE/Assault-ram-v5\":\n",
    "            state = (state / 255) - 0.5\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        n_steps = 0\n",
    "\n",
    "        # generate trajectory\n",
    "        while not done:\n",
    "            action, log_prob, _ = self.policy.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            n_steps += 1\n",
    "            if n_steps >= self.max_steps_per_episode:\n",
    "                done = True\n",
    "        returns = self._compute_returns(rewards)\n",
    "        if returns.std() > 0:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # add negative sign because Torch minimizes\n",
    "        log_probs_tensor = torch.cat(log_probs)\n",
    "        log_probs_tensor = log_probs_tensor.to(self.device)\n",
    "        policy_loss = -torch.sum(log_probs_tensor * returns)\n",
    "\n",
    "        # policy_loss = 0\n",
    "        # for log_prob, Gt in zip(log_probs, returns):\n",
    "        #     policy_loss += -log_prob * Gt\n",
    "\n",
    "        # backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.decreasing_temp:\n",
    "            self.policy.temperature *= self.temp_decay\n",
    "\n",
    "        return sum(rewards)\n",
    "\n",
    "    def _compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        Gt = 0\n",
    "        for Rt in reversed(rewards):\n",
    "            Gt = Rt + self.gamma * Gt\n",
    "            returns.insert(0, Gt)\n",
    "        returns = torch.tensor(returns, device=self.device)\n",
    "        return returns\n",
    "\n",
    "    def reset_temperature(self):\n",
    "        self.policy.temperature = self.initial_temp\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        lr_policy=1e-3,\n",
    "        lr_value=1e-3,\n",
    "        gamma=0.99,\n",
    "        temperature=1.0,\n",
    "        temp_decay=0.9999,\n",
    "        decreasing_temp=False,\n",
    "        max_steps_per_episode=500,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        if not torch.cuda.is_available():\n",
    "            self.device = \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.env = env\n",
    "        self.env_name = env.env.spec.id if env.env and env.env.spec else \"\"\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        # actor is ZNetwork\n",
    "        self.z_network = ZNetwork(self.state_dim, self.action_dim, device=self.device)\n",
    "        self.policy = BoltzmannPolicy(\n",
    "            self.z_network, temperature=temperature, device=self.device\n",
    "        )\n",
    "        self.optimizer_policy = optim.Adam(self.z_network.parameters(), lr=lr_policy)\n",
    "\n",
    "        # critic is ValueNetwork\n",
    "        self.value_network = ValueNetwork(self.state_dim, device=self.device)\n",
    "        self.optimizer_value = optim.Adam(self.value_network.parameters(), lr=lr_value)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "        if temperature == \"decreasing\":\n",
    "            self.initial_temp = 1\n",
    "            self.decreasing_temp = True\n",
    "            self.temp_decay = temp_decay\n",
    "        else:\n",
    "            self.initial_temp = temperature\n",
    "            self.decreasing_temp = decreasing_temp\n",
    "            self.temp_decay = temp_decay\n",
    "\n",
    "    def train_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        if self.env_name == \"ALE/Assault-ram-v5\":\n",
    "            state = (state / 255.0) - 0.5\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob, _ = self.policy.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # one‐step TD error (advantage)\n",
    "            # A = R + gamma * V(next_state) − V(state)\n",
    "            state_tensor = torch.tensor(\n",
    "                state, dtype=torch.float32, device=self.z_network.device\n",
    "            )\n",
    "            # value_s = self.value_network(torch.FloatTensor(state_tensor))\n",
    "            value_s = self.value_network((state_tensor))\n",
    "            if done:\n",
    "                value_s_prime = 0.0  # No future reward if episode is done\n",
    "            else:\n",
    "                next_state_tensor = torch.tensor(\n",
    "                    next_state, dtype=torch.float32, device=self.z_network.device\n",
    "                )\n",
    "                value_s_prime = self.value_network(next_state_tensor)\n",
    "\n",
    "            # TODO check why reove detech fixes\n",
    "            # advantage = (reward + self.gamma * value_s_prime - value_s).detach()\n",
    "            td_target = reward + self.gamma * value_s_prime\n",
    "            advantage = td_target - value_s\n",
    "\n",
    "            # update critic to reduce TD error to minimize (advantage)^2\n",
    "            value_loss = advantage.pow(2).mean()\n",
    "\n",
    "            self.optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "            # update actor with advantage\n",
    "            policy_loss = -log_prob * advantage.detach()\n",
    "            self.optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_policy.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if self.decreasing_temp:\n",
    "                self.policy.temperature *= self.temp_decay\n",
    "\n",
    "            if steps >= self.max_steps_per_episode:\n",
    "                break\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def reset_temperature(self):\n",
    "        self.policy.temperature = self.initial_temp\n",
    "\n",
    "\n",
    "def run_trial(env, seed, lr, temperature, algorithm, device=\"cpu\"):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    num_episodes = 1000\n",
    "\n",
    "    agent = None\n",
    "    if algorithm == \"REINFORCE\":\n",
    "        agent = REINFORCE(env, lr=lr, temperature=temperature, device=device)\n",
    "    elif algorithm == \"Actor-Critic\":\n",
    "        agent = ActorCritic(\n",
    "            env,\n",
    "            lr_policy=lr,\n",
    "            lr_value=lr,\n",
    "            gamma=0.99,\n",
    "            temperature=temperature,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    episode_rewards = []\n",
    "    for _ in tqdm.tqdm(range(num_episodes)):\n",
    "        agent.reset_temperature()\n",
    "        total_reward = agent.train_episode()\n",
    "        episode_rewards.append(total_reward)\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = run_trial(acrobot_env, 0, 1e-3, 1.0, \"Actor-Critic\", device=\"cuda\")\n",
    "\n",
    "# Create a single plot for the specific run\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the rewards for each episode\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Actor-Critic on Acrobot-v1\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a horizontal line at y=0 for reference\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = run_trial(acrobot_env, 0, 1e-3, 1.0, \"REINFORCE\", device=\"cpu\")\n",
    "\n",
    "# Create a single plot for the specific run\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the rewards for each episode\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"REINFORCE on Acrobot-v1\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a horizontal line at y=0 for reference\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = run_trial(assault_env, 0, 0.01, 1, \"REINFORCE\", device=\"cuda\")\n",
    "\n",
    "# Create a single plot for the specific run\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the rewards for each episode\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"REINFORCE on Acrobot-v1\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a horizontal line at y=0 for reference\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "2 envs, 3 epsilons, 3 step sizes, 2 algos, 2 runs w/wo buffer, 10 seeds (runs)\n",
    "=720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [0.01]\n",
    "temperatures = [1, \"decreasing\"]\n",
    "seeds = range(10)\n",
    "envs = [acrobot_env, assault_env]\n",
    "algorithms = [\"REINFORCE\", \"Actor-Critic\"]\n",
    "\n",
    "total_trials = (\n",
    "    len(envs) * len(seeds) * len(step_sizes) * len(temperatures) * len(algorithms)\n",
    ")\n",
    "\n",
    "# load pickle file for results, check if file exists\n",
    "try:\n",
    "    with open(\"results_q3.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "except:\n",
    "    results = {}\n",
    "\n",
    "trials_completed = 0\n",
    "for env in envs:\n",
    "    env_name = env.env.spec.id\n",
    "    for lr in step_sizes:\n",
    "        for temperature in temperatures:\n",
    "            for algorithm in algorithms:\n",
    "                for seed in seeds:\n",
    "                    if not (\n",
    "                        (\n",
    "                            env_name,\n",
    "                            seed,\n",
    "                            lr,\n",
    "                            temperature,\n",
    "                            algorithm,\n",
    "                        )\n",
    "                        in results.keys()\n",
    "                    ):\n",
    "                        episode_rewards = run_trial(\n",
    "                            env,\n",
    "                            seed,\n",
    "                            lr,\n",
    "                            temperature,\n",
    "                            algorithm,\n",
    "                        )\n",
    "                        results[\n",
    "                            (\n",
    "                                env_name,\n",
    "                                seed,\n",
    "                                lr,\n",
    "                                temperature,\n",
    "                                algorithm,\n",
    "                            )\n",
    "                        ] = episode_rewards\n",
    "                        with open(\"results_q3.pkl\", \"wb\") as f:\n",
    "                            pickle.dump(results, f)\n",
    "                    trials_completed += 1\n",
    "                    print(f\"Completed {trials_completed}/{total_trials} trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in [\"Acrobot-v1\", \"ALE/Assault-ram-v5\"]:\n",
    "    for use_buffer in [False, True]:\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "        fig.suptitle(f\"{env_name} - {'With' if use_buffer else 'Without'} Replay Buffer\")\n",
    "        \n",
    "        for i, epsilon in enumerate(epsilons):\n",
    "            for j, lr in enumerate(step_sizes):\n",
    "                ax = axes[i, j]\n",
    "                ax.set_title(f\"ε={epsilon}, α={lr}\")\n",
    "                ax.set_xlabel(\"Episode\")\n",
    "                ax.set_ylabel(\"Performance\")\n",
    "                \n",
    "                for algorithm, color in zip(algorithms, [\"red\", \"green\"]):\n",
    "                    all_rewards = []\n",
    "                    \n",
    "                    for seed in range(10):\n",
    "                        key = (env_name, seed, epsilon, lr, algorithm, use_buffer)\n",
    "                        if key in results:\n",
    "                            all_rewards.append(results[key])\n",
    "                        \n",
    "                    if all_rewards:\n",
    "                        all_rewards = np.array(all_rewards)\n",
    "                        mean_rewards = np.mean(all_rewards, axis=0)\n",
    "                        std_rewards = np.std(all_rewards, axis=0)\n",
    "                        \n",
    "                        ax.plot(mean_rewards, label=algorithm, color=color)\n",
    "                        ax.fill_between(range(len(mean_rewards)), \n",
    "                                        mean_rewards - std_rewards, \n",
    "                                        mean_rewards + std_rewards, \n",
    "                                        color=color, alpha=0.3)\n",
    "                \n",
    "                ax.legend()\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(f\"{env_name.replace('/', '')}_{'with' if use_buffer else 'without'}_buffer.png\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
